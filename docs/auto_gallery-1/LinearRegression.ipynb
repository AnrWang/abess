{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Linear Regression\n\nIn this tutorial, we are going to demonstrate how to use the `abess` package to carry out best subset selection \nin linear regression with both simulated data and real data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Linear Regression\n\nOur package `abess` implement a polynomial algorithm in the for best-subset selection problem:\n\n\\begin{align}\\min_{\\beta\\in \\mathbb{R}^p} \\frac{1}{2n} ||y-X\\beta||^2_2,\\quad \\text{s.t.}\\ ||\\beta||_0\\leq s,\\end{align}\n\n\nwhere $\\| \\cdot \\|_2$ is the $\\ell_2$ norm, $\\|\\beta\\|_0=\\sum_{i=1}^pI( \\beta_i\\neq 0)$ is the $\\ell_0$ norm of $\\beta$, and the sparsity level $s$ is usually an unknown non-negative integer.\nNext, we present an example to show how to use the `abess` package to solve a simple problem. \n\n### Simulated Data Example\nFixed Support Size Best Subset Selection\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\nWe generate a design matrix $X$ containing 300 observation and each observation has 1000 predictors. The response variable $y$ is linearly related to the first, second, and fifth predictors in $X$:\n .. math::\n  y = 3X_1 + 1.5X_2 + 2X_5 + \\epsilon,\n\nwhere $\\epsilon$ is a standard normal random variable. \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nfrom abess.datasets import make_glm_data\nnp.random.seed(0)\n\nn = 300\np = 1000\nk = 3\nreal_coef = np.zeros(p)\nreal_coef[[0, 1, 4]] = 3, 1.5, 2\ndata1 = make_glm_data(n = n, p = p, k = k, family = \"gaussian\", coef_ = real_coef)\n\n\nprint(data1.x.shape)\nprint(data1.y.shape)\n\nimport matplotlib.pyplot as plt\n\n_ = plt.plot([1,2,3])\n# # Use `LinearRegression` to fit the data, with a fixed support size:\n\n# from abess import LinearRegression\n# model = LinearRegression(support_size = 3)\n# model.fit(data1.x, data1.y)\n\n\n# # After fitting, the predicted coefficients are stored in `model.coef_`:\n\n# print(\"shape:\", model.coef_.shape)\n\n# #%%\n\n# ind = np.nonzero(model.coef_)\n# print(\"predicted non-zero: \", ind)\n# print(\"predicted coef: \", model.coef_[ind])\n\n# #%%\n# # From the result, we know that `abess` found which 3 predictors are useful among all 1000 variables. Besides, the predicted coefficients of them are quite close to the real ones. \n# # \n# # Adaptive Best Subset Selection\n# # \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n# # However, we may not know the true sparsity level in real world data, and thus we need to determine the most proper one from a large range. Suppose that we believe the real sparsity level is between 0 to 30 (so that `range(0, 31)`):\n\n\n# model = LinearRegression(support_size = range(31))\n# model.fit(data1.x, data1.y)\n\n# ind = np.nonzero(model.coef_)\n# print(\"predicted non-zero: \", ind)\n# print(\"predicted coef: \", model.coef_[ind])\n\n# #%%\n# # The program can adaptively choose the sparsity level that best fit the data. It is not surprising that it choose 3 variables, the same as the last section. \n\n# ###############################################################################\n# # Real data example\n# # ------------------------\n# #\n# # Hitters Dataset\n# # ~~~~~~~~~~~~~~~~~~~~~~\n# # Now we focus on real data on the `Hitters` dataset: [https://www.kaggle.com/floser/hitters](https://www.kaggle.com/floser/hitters).\n# # We hope to use sevral predictors related to the performance of the baseball atheltes last year to predict their salary.\n# # \n# # First, let's have a look at this dataset. There are 19 variables except `Salary` and 322 observations.\n\n\n# import pandas as pd\n\n# data2 = pd.read_csv('./Hitters.csv')\n# print(data2.shape)\n\n\n# print(data2.head(5))\n\n# #%%\n# # Since the dataset contains some missing values, we simply drop those rows with missing values. Then we have 263 observations remains:\n\n\n# data2 = data2.dropna()\n# print(data2.shape)\n\n# #%%\n# # What is more, before fitting, we need to transfer the character variables to dummy variables: \n\n\n# data2 = pd.get_dummies(data2)\n# data2 = data2.drop(['League_A', 'Division_E', 'NewLeague_A'], axis = 1)\n# print(data2.shape)\n# print(data2.head(5))\n\n# ###############################################################################\n# # Model Fitting\n# # ~~~~~~~~~~~~~~~~~~~~~~\n# # As what we do in simulated data, an adaptive best subset can be formed easily:\n\n# x = np.array(data2.drop('Salary', axis = 1))\n# y = np.array(data2['Salary'])\n\n# model = LinearRegression(support_size = range(20))\n# model.fit(x, y)\n\n\n# # The result can be showed:\n\n\n# ind = np.nonzero(model.coef_)\n# print(\"non-zero:\\n\", data2.columns[ind])\n# print(\"coef:\\n\", model.coef_)\n\n# #%%\n# # Automatically, variables $Hits$, $CRBI$, $PutOuts$, $League\\_N$ are chosen in the model (the chosen sparsity level is 4).\n\n# ###############################################################################\n# # More on the results\n# # ~~~~~~~~~~~~~~~~~~~~~~\n# # We can also plot the path of abess process:\n\n\n# import matplotlib.pyplot as plt\n\n# coef = np.zeros((20, 19))\n# ic = np.zeros(20)\n# for s in range(20):\n#     model = LinearRegression(support_size = s)\n#     model.fit(x, y)\n#     coef[s, :] = model.coef_\n#     ic[s] = model.ic_\n\n# for i in range(19):\n#     plt.plot(coef[:, i], label = i)\n\n# plt.xlabel('support_size')\n# plt.ylabel('coefficients')\n# # plt.legend() # too long to plot\n# plt.show()\n\n# #%%\n# # Besides, we can also generate a graph about the tuning value. Remember that we used the default EBIC to tune the support size.\n\n# plt.plot(ic, 'o-')\n# plt.xlabel('support_size')\n# plt.ylabel('EBIC')\n# plt.show()\n\n# #%%\n# # In EBIC criterion, `support_size = 4` has the lowest value, so the process adaptively choose 4 variables. Note that under other information criterion, the result may be different. \n\n# ###############################################################################\n# # R tutorial \n# # ~~~~~~~~~~~~~~~~~~~~~~\n# # For R tutorial, please view [https://abess-team.github.io/abess/articles/v01-abess-guide.html](https://abess-team.github.io/abess/articles/v01-abess-guide.html)."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}