
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_gallery\glm\plot_1_LinearRegression.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_auto_gallery_glm_plot_1_LinearRegression.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_gallery_glm_plot_1_LinearRegression.py:


======================================================
Linear Regression
======================================================


In this tutorial, we are going to demonstrate how to use the `abess` package to carry out best subset selection 
in linear regression with both simulated data and real data.

.. GENERATED FROM PYTHON SOURCE LINES 13-34

Linear Regression
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Our package `abess` implements a polynomial algorithm in the following best-subset selection problem:

.. math::
  \min_{\beta\in \mathbb{R}^p} \frac{1}{2n} ||y-X\beta||^2_2,\quad \text{s.t.}\ ||\beta||_0\leq s,


where :math:`\| \cdot \|_2` is the :math:`\ell_2` norm, :math:`\|\beta\|_0=\sum_{i=1}^pI( \beta_i\neq 0)` is the :math:`\ell_0` norm of :math:`\beta`, and the sparsity level :math:`s` is usually an unknown non-negative integer.
Next, we present an example to show how to use the `abess` package to solve a simple problem. 

Simulated Data Example
~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Fixed Support Size Best Subset Selection
""""""""""""""""""""""""""""""""""""""""""""""
We generate a design matrix :math:`X` containing 300 observations and each observation has 1000 predictors. The response variable $y$ is linearly related to the first, second, and fifth predictors in $X$:
 .. math::
  y = 3X_1 + 1.5X_2 + 2X_5 + \epsilon,

where :math:`\epsilon` is a standard normal random variable. 

.. GENERATED FROM PYTHON SOURCE LINES 34-50

.. code-block:: default



    import numpy as np
    from abess.datasets import make_glm_data
    np.random.seed(0)

    n = 300
    p = 1000
    k = 3
    real_coef = np.zeros(p)
    real_coef[[0, 1, 4]] = 3, 1.5, 2
    data1 = make_glm_data(n = n, p = p, k = k, family = "gaussian", coef_ = real_coef)


    print(data1.x.shape)
    print(data1.y.shape)




.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    (300, 1000)
    (300,)




.. GENERATED FROM PYTHON SOURCE LINES 51-52

Use `LinearRegression` to fit the data, with a fixed support size:

.. GENERATED FROM PYTHON SOURCE LINES 52-57

.. code-block:: default


    from abess import LinearRegression
    model = LinearRegression(support_size = 3)
    model.fit(data1.x, data1.y)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    LinearRegression(always_select=[], support_size=3)



.. GENERATED FROM PYTHON SOURCE LINES 58-59

After fitting, the predicted coefficients are stored in `model.coef_`:

.. GENERATED FROM PYTHON SOURCE LINES 59-65

.. code-block:: default


    print("shape:", model.coef_.shape)
    ind = np.nonzero(model.coef_)
    print("predicted non-zero: ", ind)
    print("predicted coef: ", model.coef_[ind])





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    shape: (1000,)
    predicted non-zero:  (array([0, 1, 4], dtype=int64),)
    predicted coef:  [3.04061713 1.66443756 1.90914766]




.. GENERATED FROM PYTHON SOURCE LINES 66-71

From the result, we know that `abess` found which 3 predictors are useful among all 1000 variables. Besides, the predicted coefficients of them are quite close to the real ones. 

Adaptive Best Subset Selection
""""""""""""""""""""""""""""""""""""""""""""""
However, we may not know the true sparsity level in real world data, and thus we need to determine the most proper one from a large range. Suppose that we believe the real sparsity level is between 0 and 30 (so that `range(0, 31)`):

.. GENERATED FROM PYTHON SOURCE LINES 71-80

.. code-block:: default



    model = LinearRegression(support_size = range(31))
    model.fit(data1.x, data1.y)

    ind = np.nonzero(model.coef_)
    print("predicted non-zero: ", ind)
    print("predicted coef: ", model.coef_[ind])





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    predicted non-zero:  (array([0, 1, 4], dtype=int64),)
    predicted coef:  [3.04061713 1.66443756 1.90914766]




.. GENERATED FROM PYTHON SOURCE LINES 81-82

The program can adaptively choose the sparsity level that best fits the data. It is not surprising that it chooses 3 variables, the same as the last section. 

.. GENERATED FROM PYTHON SOURCE LINES 84-93

Real data example
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Hitters Dataset
~~~~~~~~~~~~~~~~~~~~~~
Now we focus on real data on the `Hitters` dataset: [https://www.kaggle.com/floser/hitters](https://www.kaggle.com/floser/hitters).
We hope to use sevral predictors related to the performance of the baseball atheltes last year to predict their salary.

First, let's have a look at this dataset. There are 19 variables except `Salary` and 322 observations.

.. GENERATED FROM PYTHON SOURCE LINES 93-104

.. code-block:: default



    import pandas as pd
    import os

    data2 = pd.read_csv(os.path.join(os.getcwd(), 'Hitters.csv'))
    print(data2.shape)


    print(data2.head(5))





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    (322, 20)
       AtBat  Hits  HmRun  Runs  RBI  Walks  Years  CAtBat  CHits  CHmRun  CRuns  CRBI  CWalks League Division  PutOuts  Assists  Errors  Salary NewLeague
    0    293    66      1    30   29     14      1     293     66       1     30    29      14      A        E      446       33      20     NaN         A
    1    315    81      7    24   38     39     14    3449    835      69    321   414     375      N        W      632       43      10   475.0         N
    2    479   130     18    66   72     76      3    1624    457      63    224   266     263      A        W      880       82      14   480.0         A
    3    496   141     20    65   78     37     11    5628   1575     225    828   838     354      N        E      200       11       3   500.0         N
    4    321    87     10    39   42     30      2     396    101      12     48    46      33      N        E      805       40       4    91.5         N




.. GENERATED FROM PYTHON SOURCE LINES 105-106

Since the dataset contains some missing values, we simply drop those rows with missing values. Then we have 263 observations remain:

.. GENERATED FROM PYTHON SOURCE LINES 106-111

.. code-block:: default



    data2 = data2.dropna()
    print(data2.shape)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    (263, 20)




.. GENERATED FROM PYTHON SOURCE LINES 112-113

What is more, before fitting, we need to transfer the character variables to dummy variables: 

.. GENERATED FROM PYTHON SOURCE LINES 113-120

.. code-block:: default



    data2 = pd.get_dummies(data2)
    data2 = data2.drop(['League_A', 'Division_E', 'NewLeague_A'], axis = 1)
    print(data2.shape)
    print(data2.head(5))





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    (263, 20)
       AtBat  Hits  HmRun  Runs  RBI  Walks  Years  CAtBat  CHits  CHmRun  CRuns  CRBI  CWalks  PutOuts  Assists  Errors  Salary  League_N  Division_W  NewLeague_N
    1    315    81      7    24   38     39     14    3449    835      69    321   414     375      632       43      10   475.0         1           1            1
    2    479   130     18    66   72     76      3    1624    457      63    224   266     263      880       82      14   480.0         0           1            0
    3    496   141     20    65   78     37     11    5628   1575     225    828   838     354      200       11       3   500.0         1           0            1
    4    321    87     10    39   42     30      2     396    101      12     48    46      33      805       40       4    91.5         1           0            1
    5    594   169      4    74   51     35     11    4408   1133      19    501   336     194      282      421      25   750.0         0           1            0




.. GENERATED FROM PYTHON SOURCE LINES 121-124

Model Fitting
~~~~~~~~~~~~~~~~~~~~~~
As what we do in simulated data, an adaptive best subset can be formed easily:

.. GENERATED FROM PYTHON SOURCE LINES 124-139

.. code-block:: default


    x = np.array(data2.drop('Salary', axis = 1))
    y = np.array(data2['Salary'])

    model = LinearRegression(support_size = range(20))
    model.fit(x, y)


    # The result can is shown as follows:


    ind = np.nonzero(model.coef_)
    print("non-zero:\n", data2.columns[ind])
    print("coef:\n", model.coef_)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    non-zero:
     Index(['Hits', 'CRBI', 'PutOuts', 'League_N'], dtype='object')
    coef:
     [   0.            2.67579779    0.            0.            0.
        0.            0.            0.            0.            0.
        0.            0.681779      0.            0.27350022    0.
        0.            0.         -139.9538855     0.        ]




.. GENERATED FROM PYTHON SOURCE LINES 140-141

Automatically, variables $Hits$, $CRBI$, $PutOuts$, $League\_N$ are chosen in the model (the chosen sparsity level is 4).

.. GENERATED FROM PYTHON SOURCE LINES 143-146

More on the results
~~~~~~~~~~~~~~~~~~~~~~
We can also plot the path of abess process:

.. GENERATED FROM PYTHON SOURCE LINES 146-166

.. code-block:: default



    import matplotlib.pyplot as plt

    coef = np.zeros((20, 19))
    ic = np.zeros(20)
    for s in range(20):
        model = LinearRegression(support_size = s)
        model.fit(x, y)
        coef[s, :] = model.coef_
        ic[s] = model.ic_

    for i in range(19):
        plt.plot(coef[:, i], label = i)

    plt.xlabel('support_size')
    plt.ylabel('coefficients')
    # plt.legend() # too long to plot
    plt.show()




.. image-sg:: /auto_gallery/glm/images/sphx_glr_plot_1_LinearRegression_001.png
   :alt: plot 1 LinearRegression
   :srcset: /auto_gallery/glm/images/sphx_glr_plot_1_LinearRegression_001.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 167-168

Besides, we can also generate a graph about the tuning parameter. Remember that we used the default EBIC to tune the support size.

.. GENERATED FROM PYTHON SOURCE LINES 168-174

.. code-block:: default


    plt.plot(ic, 'o-')
    plt.xlabel('support_size')
    plt.ylabel('EBIC')
    plt.show()




.. image-sg:: /auto_gallery/glm/images/sphx_glr_plot_1_LinearRegression_002.png
   :alt: plot 1 LinearRegression
   :srcset: /auto_gallery/glm/images/sphx_glr_plot_1_LinearRegression_002.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 175-176

In EBIC criterion, `support_size = 4` has the lowest value, so the process adaptively chooses 4 variables. Note that under other information criteria, the result may be different. 

.. GENERATED FROM PYTHON SOURCE LINES 178-181

R tutorial 
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
For R tutorial, please view [https://abess-team.github.io/abess/articles/v01-abess-guide.html](https://abess-team.github.io/abess/articles/v01-abess-guide.html).


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  0.491 seconds)


.. _sphx_glr_download_auto_gallery_glm_plot_1_LinearRegression.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: plot_1_LinearRegression.py <plot_1_LinearRegression.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: plot_1_LinearRegression.ipynb <plot_1_LinearRegression.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
