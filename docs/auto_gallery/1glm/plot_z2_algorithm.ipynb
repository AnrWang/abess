{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# ABESS algorithm: details\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction \n\nThe ABESS algorithm employing \"splicing\" technique can exactly solve\ngeneral best subset problem in a polynomial time. The aim of this page\nto provide a complete and coherent documentation for ABESS algorithm\nsuch that users can easily understand the ABESS algorithm and its\nvariants, thereby facilitating the usage of ``abess`` software.\n\n## linear regression \n\n\n### Sacrifices\n\nConsider the $\\ell_{0}$ constraint minimization problem,\n\n\\begin{align}\\min _{\\boldsymbol{\\beta}} \\mathcal{L}_{n}(\\beta), \\quad \\text { s.t }\\|\\boldsymbol{\\beta}\\|_{0} \\leq \\mathrm{s},\\end{align}\n\nwhere\n$\\mathcal{L}_{n}(\\boldsymbol \\beta)=\\frac{1}{2 n}\\|y-X \\beta\\|_{2}^{2} .$\nWithout loss of generality, we consider\n$\\|\\boldsymbol{\\beta}\\|_{0}=\\mathrm{s}$. Given any initial set\n$\\mathcal{A} \\subset \\mathcal{S}=\\{1,2, \\ldots, p\\}$ with\ncardinality $|\\mathcal{A}|=s$, denote\n$\\mathcal{I}=\\mathcal{A}^{\\mathrm{c}}$ and compute:\n\n\\begin{align}\\hat{\\boldsymbol{\\beta}}=\\arg \\min _{\\boldsymbol{\\beta}_{\\mathcal{I}}=0} \\mathcal{L}_{n}(\\boldsymbol{\\beta}).\\end{align}\n\nWe call $\\mathcal{A}$ and $\\mathcal{I}$ as the active set\nand the inactive set, respectively.\n\nGiven the active set $\\mathcal{A}$ and\n$\\hat{\\boldsymbol{\\beta}}$, we can define the following two types\nof sacrifices:\n\n1. Backward sacrifice: For any $j \\in \\mathcal{A}$, the magnitude\nof discarding variable $j$ is,\n\n\\begin{align}\\xi_{j}=\\mathcal{L}_{n}\\left(\\hat{\\boldsymbol{\\beta}}^{\\mathcal{A} \\backslash\\{j\\}}\\right)-\\mathcal{L}_{n}\\left(\\hat{\\boldsymbol{\\beta}}^{\\mathcal{A}}\\right)=\\frac{X_{j}^{\\top} X_{j}}{2 n}\\left(\\hat{\\boldsymbol\\beta}_{j}\\right)^{2},\\end{align}\n\n2. Forward sacrifice: For any $j \\in \\mathcal{I}$, the magnitude\nof adding variable $j$ is,\n\n\\begin{align}\\zeta_{j}=\\mathcal{L}_{n}\\left(\\hat{\\boldsymbol{\\beta}^{\\mathcal{A}}}\\right)-\\mathcal{L}_{n}\\left(\\hat{\\boldsymbol{\\beta}}^{\\mathcal{A}}+\\hat{t}^{\\{j\\}}\\right)=\\frac{X_{j}^{\\top} X_{j}}{2 n}\\left(\\frac{\\hat{\\boldsymbol d}_{j}}{X_{j}^{\\top} X_{j} / n}\\right)^{2}.\\end{align}\n\n| where\n  $\\hat{t}=\\arg \\min _{t} \\mathcal{L}_{n}\\left(\\hat{\\boldsymbol{\\beta}}^{\\mathcal{A}}+t^{\\{j\\}}\\right), \\hat{\\boldsymbol d}_{j}=X_{j}^{\\top}(y-X \\hat{\\boldsymbol{\\beta}}) / n$.\n  Intuitively, for $j \\in \\mathcal{A}$ (or\n  $j \\in \\mathcal{I}$ ), a large $\\xi_{j}$ (or\n  $\\zeta_{j}$) implies the $j$ th variable is potentially\n  important.\n\n\n### Algorithm\n\n\n#### Best-Subset Selection with a Given Support Size\n\nUnfortunately, it is noteworthy that these two sacrifices are\nincomparable because they have different sizes of support set. However,\nif we exchange some \"irrelevant\" variables in $\\mathcal{A}$ and\nsome \"important\" variables in $\\mathcal{I}$, it may result in a\nhigher-quality solution. This intuition motivates our splicing method.\nSpecifically, given any splicing size $k \\leq s$, define\n\n\\begin{align}\\mathcal{A}_{k}=\\left\\{j \\in \\mathcal{A}: \\sum_{i \\in \\mathcal{A}} \\mathrm{I}\\left(\\xi_{j} \\geq \\xi_{i}\\right) \\leq k\\right\\},\\end{align}\n\nto represent $k$ least relevant variables in $\\mathcal{A}$\nand,\n\n\\begin{align}\\mathcal{I}_{k}=\\left\\{j \\in \\mathcal{I}: \\sum_{i \\in \\mathcal{I}} \\mid\\left(\\zeta_{j} \\leq \\zeta_{i}\\right) \\leq k\\right\\},\\end{align}\n\nto represent $k$ most relevant variables in $\\mathcal{I} .$\n\n| Then, we splice $\\mathcal{A}$ and $\\mathcal{I}$ by\n  exchanging $\\mathcal{A}_{k}$ and $\\mathcal{I}_{k}$ and\n  obtain a new active\n  set:$\\tilde{\\mathcal{A}}=\\left(\\mathcal{A} \\backslash \\mathcal{A}_{k}\\right) \\cup \\mathcal{I}_{k}.$\n  Let\n  $\\tilde{\\mathcal{I}}=\\tilde{\\mathcal{A}}^{c}, \\tilde{\\boldsymbol{\\beta}}=\\arg \\min _{\\boldsymbol{\\beta}_{\\overline{\\mathcal{I}}=0}} \\mathcal{L}_{n}(\\boldsymbol{\\beta})$,\n  and $\\tau_{s}>0$ be a threshold. If $\\tau_{s}<\\mathcal{L}_{n}(\\hat{\\boldsymbol\\beta})-\\mathcal{L}_{n}(\\tilde{\\boldsymbol\\beta})$,\n  then $\\tilde{A}$ is preferable to $\\mathcal{A} .$ \n| The\n  active set can be updated\n  iteratively until the loss function cannot be improved by splicing.\n  Once the algorithm recovers the true active set, we may splice some\n  irrelevant variables, and then the loss function may decrease\n  slightly. The threshold $\\tau_{s}$ can reduce this unnecessary\n  calculation. Typically, $\\tau_{s}$ is relatively small, e.g.\n  $\\tau_{s}=0.01 s \\log (p) \\log (\\log n) / n.$\n\n\n##### Algorithm 1: BESS.Fix(s): Best-Subset Selection with a given support size $s$.\n\n1. Input: $X, y$, a positive integer $k_{\\max }$, and a\n   threshold $\\tau_{s}$.\n\n2. Initialize: \n\n\\begin{align}\\mathcal{A}^{0}=\\left\\{j: \\sum_{i=1}^{p} \\mathrm{I}\\left(\\left|\\frac{X_{j}^{\\top} y}{\\sqrt{X_{j}^{\\top} X_{j}}}\\right| \\leq \\left| \\frac{X_{i}^{\\top} y}{\\sqrt{X_{i}^{\\top} X_{i}}}\\right| \\leq \\mathrm{s}\\right\\}, \\mathcal{I}^{0}=\\left(\\mathcal{A}^{0}\\right)^{c}\\right.\\end{align}\n\nand $\\left(\\boldsymbol\\beta^{0}, d^{0}\\right):$\n\n\\begin{align}&\\boldsymbol{\\beta}_{\\mathcal{I}^{0}}^{0}=0,\\\\\n         &d_{\\mathcal{A}^{0}}^{0}=0,\\\\\n      &\\boldsymbol{\\beta}_{\\mathcal{A}^{0}}^{0}=\\left(\\boldsymbol{X}_{\\mathcal{A}^{0}}^{\\top} \\boldsymbol{X}_{\\mathcal{A}^{0}}\\right)^{-1} \\boldsymbol{X}_{\\mathcal{A}^{0}}^{\\top} \\boldsymbol{y},\\\\\n      &d_{\\mathcal{I}^{0}}^{0}=X_{\\mathcal{I}^{0}}^{\\top}\\left(\\boldsymbol{y}-\\boldsymbol{X} \\boldsymbol{\\beta}^{0}\\right).\\end{align}\n\n3. For $m=0,1, \\ldots$, do\n\n      .. math:: \\left(\\boldsymbol{\\beta}^{m+1}, \\boldsymbol{d}^{m+1}, \\mathcal{A}^{m+1}, \\mathcal{I}^{m+1}\\right)= \\text{Splicing} \\left(\\boldsymbol{\\beta}^{m}, \\boldsymbol{d}^{m}, \\mathcal{A}^{m}, \\mathcal{I}^{m}, k_{\\max }, \\tau_{s}\\right).\n\n      If $\\left(\\mathcal{A}^{m+1}, \\mathcal{I}^{m+1}\\right)=\\left(\\mathcal{A}^{m},\\mathcal{I}^{m}\\right)$,\n      then stop.\n\n   End For\n\n4. Output\n   $(\\hat{\\boldsymbol{\\beta}}, \\hat{\\boldsymbol{d}}, \\hat{\\mathcal{A}}, \\hat{\\mathcal{I}})=\\left(\\boldsymbol{\\beta}^{m+1}, \\boldsymbol{d}^{m+1} \\mathcal{A}^{m+1}, \\mathcal{I}^{m+1}\\right).$\n\n\n##### Algorithm 2: Splicing $\\left(\\boldsymbol\\beta, d, \\mathcal{A}, \\mathcal{I}, k_{\\max }, \\tau_{s}\\right)$\n\n1. Input:\n   $\\boldsymbol{\\beta}, \\boldsymbol{d}, \\mathcal{A}, \\mathcal{I}, k_{\\max }$,\n   and $\\tau_{\\mathrm{s}} .$\n\n2. Initialize: \n   $L_{0}=L=\\frac{1}{2 n}\\|y-X \\beta\\|_{2}^{2}$, and set\n\n   .. math:: \\xi_{j}=\\frac{X_{j}^{\\top} X_{j}}{2 n}\\left(\\beta_{j}\\right)^{2}, \\zeta_{j}=\\frac{X_{j}^{\\top} X_{j}}{2 n}\\left(\\frac{d_{j}}{X_{j}^{\\top} X_{j} / n}\\right)^{2}, j=1, \\ldots, p.\n\n3. For $k=1,2, \\ldots, k_{\\max }$, do\n\n      .. math::\n\n         \\mathcal{A}_{k}=\\left\\{j \\in \\mathcal{A}: \\sum_{i \\in \\mathcal{A}} \\mathrm{I}\\left(\\xi_{j} \\geq \\xi_{i}\\right) \\leq k\\right\\},\\\\\n         \\mathcal{I}_{k}=\\left\\{j \\in \\mathcal{I}: \\sum_{i \\in \\mathcal{I}} \\mathrm{I}\\left(\\zeta_{j} \\leq \\zeta_{i}\\right) \\leq k\\right\\}.\n\n      Let\n      $\\tilde{\\mathcal{A}}_{k}=\\left(\\mathcal{A} \\backslash \\mathcal{A}_{k}\\right) \\cup \\mathcal{I}_{k}, \\tilde{\\mathcal{I}}_{k}=\\left(\\mathcal{I} \\backslash \\mathcal{I}_{k}\\right) \\cup \\mathcal{A}_{k}$\n      and solve:\n\n      .. math::\n\n         \\tilde{\\boldsymbol{\\beta}}_{{\\mathcal{A}}_{k}}=\\left(\\boldsymbol{X}_{\\mathcal{A}_{k}}^{\\top} \\boldsymbol{X}_{{\\mathcal{A}}_{k}}\\right)^{-1} \\boldsymbol{X}_{{\\mathcal{A}_{k}}}^{\\top} y, \\quad \\tilde{\\boldsymbol{\\beta}}_{{\\mathcal{I}}_{k}}=0\\\\\n         \\tilde{\\boldsymbol d}_{\\mathcal{I}^k}=X_{\\mathcal{I}^k}^{\\top}(y-X \\tilde{\\beta}) / n,\\quad \\tilde{\\boldsymbol d}_{\\mathcal{A}^k} = 0.\n\n      Compute:\n      $\\mathcal{L}_{n}(\\tilde{\\boldsymbol\\beta})=\\frac{1}{2 n}\\|y-X \\tilde{\\boldsymbol\\beta}\\|_{2}^{2}.$\n      If $L>\\mathcal{L}_{n}(\\tilde{\\boldsymbol\\beta})$, then\n\n      .. math::\n\n         (\\hat{\\boldsymbol{\\beta}}, \\hat{\\boldsymbol{d}}, \\hat{\\mathcal{A}}, \\hat{\\mathcal{I}})=\\left(\\tilde{\\boldsymbol{\\beta}}, \\tilde{\\boldsymbol{d}}, \\tilde{\\mathcal{A}}_{k}, \\tilde{\\mathcal{I}}_{k}\\right)\\\\\n         L=\\mathcal{L}_{n}(\\tilde{\\boldsymbol\\beta}).\n\n   End for\n\n3. If $L_{0}-L<\\tau_{s}$, then\n   $(\\hat{\\boldsymbol\\beta}, \\hat{d}, \\hat{A}, \\hat{I})=(\\boldsymbol\\beta, d, \\mathcal{A}, \\mathcal{I}).$\n\n2. Output\n   $(\\hat{\\boldsymbol{\\beta}}, \\hat{\\boldsymbol{d}}, \\hat{\\mathcal{A}}, \\hat{\\mathcal{I}})$.\n\n#### Determining the Best Support Size with SIC\n\nIn practice, the support size is usually unknown. We use a datadriven\nprocedure to determine s. For any active set $\\mathcal{A}$, define\nan $\\mathrm{SIC}$ as follows:\n\n\\begin{align}\\operatorname{SIC}(\\mathcal{A})=n \\log \\mathcal{L}_{\\mathcal{A}}+|\\mathcal{A}| \\log (p) \\log \\log n,\\end{align}\n\nwhere\n$\\mathcal{L}_{\\mathcal{A}}=\\min _{\\beta_{\\mathcal{I}}=0} \\mathcal{L}_{n}(\\beta), \\mathcal{I}=(\\mathcal{A})^{c}$.\nTo identify the true model, the model complexity penalty is\n$\\log p$ and the slow diverging rate $\\log \\log n$ is set to\nprevent underfitting. Theorem 4 states that the following ABESS\nalgorithm selects the true support size via SIC.\n\nLet $s_{\\max }$ be the maximum support size. We suggest\n$s_{\\max }=o\\left(\\frac{n}{\\log p}\\right)$ as the maximum possible\nrecovery size. Typically, we set\n$s_{\\max }=\\left[\\frac{n}{\\log p \\log \\log n}\\right]$ where\n$[x]$ denotes the integer part of $x$.\n\n\n##### Algorithm 3: ABESS.\n\n1. Input: $X, y$, and the maximum support size $s_{\\max } .$\n\n2. For $s=1,2, \\ldots, s_{\\max }$, do\n\n   .. math:: \\left(\\hat{\\boldsymbol{\\beta}}_{s}, \\hat{\\boldsymbol{d}}_{s}, \\hat{\\mathcal{A}}_{s}, \\hat{\\mathcal{I}}_{s}\\right)= \\text{BESS.Fixed}(s).\n\n   End for\n\n3. Compute the minimum of SIC:\n\n   .. math:: s_{\\min }=\\arg \\min _{s} \\operatorname{SIC}\\left(\\hat{\\mathcal{A}}_{s}\\right).\n\n4. Output\n   $\\left(\\hat{\\boldsymbol{\\beta}}_{s_{\\min}}, \\hat{\\boldsymbol{d}}_{s_{\\min }}, \\hat{A}_{s_{\\min }}, \\hat{\\mathcal{I}}_{s_{\\min }}\\right) .$\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}