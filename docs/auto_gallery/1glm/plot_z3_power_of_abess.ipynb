{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Power of abess\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# sphinx_gallery_thumbnail_path = '_static/timings.png'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this part, we are going to explore the power of the abess package\nusing simulated data. We compare the abess package with popular Python\npackages:\n`scikit-learn <https://scikit-learn.org/stable/supervised_learning.html#supervised-learning>`__\nfor linear and logistic regressions in the following section. Actually,\nwe also compare with\n`python-glmnet <https://github.com/civisanalytics/python-glmnet>`__,\n`statsmodels <https://github.com/statsmodels/statsmodels>`__ and\n`L0bnb <https://github.com/alisaab/l0bnb>`__, but the python-glmnet\npresents a poor prediction error, the statsmodels runs slow and the\nL0bnb cannot adaptively choose sparsity level. So their results are not\nshowed here.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Simulation\nSetting\n~~~~~~~\nBoth packages are compared in three aspects including the prediction\nperformance, the variable selection performance, and the computation\nefficiency.\n\n-  The prediction performance of the linear model is measured by\n   $||y\u2212\\hat{y}||_2$ on a test set and for logistic regression\n   this is measured by the area under the ROC Curve (AUC).\n-  For the variable selection performance, we compute the coefficient\n   error $||\\beta - \\hat{\\beta}||_2$, true positive rate (TPR,\n   which is the proportion of varibales in the active set that are\n   correctly identified) and the false positive rate (FPR, which is the\n   proportion of the varibales in the inactive set that are falsely\n   identified as a signal).\n-  Timings of the CPU execution are recorded in seconds and all the\n   performances are averaged over 20 replications on a sequence of 100\n   regularization parameters.\nThe simulated data are made by ``abess.datasets.make_glm_data()``. The\nnumber of predictors is $p=8000$ and the size of data is\n$n=500$. The true coefficient contains $k=10$ nonzero\nentries uniformly distributed in $[b,B]$. For linear (gaussian)\ndata, we set $b = 5\\sqrt{2\\ln p / n}$ and $B = 100b$. For\nlogistic (binomial) data, we set $b = 10\\sqrt{2\\ln p / n}$ and\n$B = 5b$. In each regression, we test for both low\n($\\rho=0.1$) and high correlation ($\\rho=0.7$) scenarios.\nWhat\u2019s more, a random noise generated from a standard Gaussian\ndistribution is added to the linear predictor $x\u2032\u03b2$ for linear\nregression.\n\nAll experiments are evaluated on a Ubuntu platform with Intel(R)\nCore(TM) i9-9940X CPU @ 3.30GHz and 48 RAM.\n\n.. code:: bash\n\n   $ python abess/docs/simulation/Python/perform.py\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Results\n\n   For linear regression, we compare three methods in the two packages:\n   Lasso, OMP and abess. For logistic regression, we compare two\n   methods: lasso and abess.\nThe results are presented in the following pictures. The first column is\nthe result of linear regression and the second one is of logistic\nregression.\n-  Firstly, among all of the methods implemented in different packages,\n   the estimator obtained by the abess package shows both the best\n   prediction performance and the best coefficient error.\n-  Secondly, the estimator obtained by the abess package can reasonably\n   control FPR in a low level while the TPR stays at 1. (Since all\n   methods\u2019 TPR are 1, the figure is not plotted.)\n-  Furthermore, our abess package is highly efficient compared with\n   other packages, especially in the linear regression.\n|image0|\n|image1|\n\n### R performance\nFor R performance, please view\nhttps://abess-team.github.io/abess/articles/v11-power-of-abess.html.\n\n.. |image0| image:: ./images/perform.png\n.. |image1| image:: ./images/timings.png\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}