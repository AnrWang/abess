{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# p>>N (Important Search)\nSuppose that there are only a few variables are important (i.e. too many noise variables), it may be a vise choice to focus on some important variables in splicing process. This can save a lot of time, especially under a large $p$.\n\nIn abess package, an argument called `important_search` is used for it, which means the size of inactive set for each splicing process. By default, this argument is set as 0, and the total inactive variables would be contained in the inactive set. But if an positive integer is given, the splicing process would focus on active set and the most important `important_search` inactive variables.\n \nHowever, after convergence on this subset, we check if the chosen variables are still the most important ones by recomputing on the full set with the new active set. If not, we update the subset and splicing again. On our testing, it would not iterate many time to reach a stable subset. After that, the active set on the stable subset would be treated as that on the full set.\n\nHere we take `LogisticRegression` for an example. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from abess.linear import LogisticRegression\nfrom abess.datasets import make_glm_data\nfrom time import time\nimport numpy as np\n\ndata = make_glm_data(n = 500, p = 10000, k = 10, family = \"binomial\")\n\nt1 = time()\nmodel = LogisticRegression()\nmodel.fit(data.x, data.y)\nt2 = time()\n\nprint(\"non_zero :\\n\", np.nonzero(model.coef_)[0])\nprint(\"time : \", t2 - t1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "However, if we only focus on 500 important inactive variables when searching:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "t1 = time()\nmodel2 = LogisticRegression(important_search = 500)\nmodel2.fit(data.x, data.y)\nt2 = time()\n\nprint(\"non_zero :\\n\", np.nonzero(model2.coef_)[0])\nprint(\"time : \", t2 - t1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It takes much less time to reach the same result. We recommend use this method for large $p$ situation, but in small one, it may not be faster than the primary fitting.\n\nHere we compare the AUC and runtime for `LogisticRegression` under different `important_search` and the test code can be found [here](https://github.com/abess-team/abess/blob/master/docs/simulation/Python/impsearch.py).\n\n\n<img src=\"file://./images/impsearch.png\">\nAt a low level of `important_search`, however, the performance (AUC) has been very good. In this situation, a lower `important_search` can save lots of time and space.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# R tutorial\n\nFor R tutorial, please view [https://abess-team.github.io/abess/articles/v09-fasterSetting.html](https://abess-team.github.io/abess/articles/v09-fasterSetting.html).\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}