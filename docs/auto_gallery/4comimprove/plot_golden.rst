
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_gallery\4comimprove\plot_golden.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_auto_gallery_4comimprove_plot_golden.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_gallery_4comimprove_plot_golden.py:


Golden-section searching
=========================

.. GENERATED FROM PYTHON SOURCE LINES 6-7

Here we generate a simple example and draw the path of scores of information criterion. Typically, the curve should be a strictly unimodal function achieving minimum at the true subset size.

.. GENERATED FROM PYTHON SOURCE LINES 7-28

.. code-block:: default


    import numpy as np
    import matplotlib.pyplot as plt
    from abess.datasets import make_glm_data
    from abess.linear import LinearRegression

    np.random.seed(0)
    data = make_glm_data(n = 100, p = 20, k = 5, family = 'gaussian')

    ic = np.zeros(21)
    for sz in range(21):
        model = LinearRegression(support_size = [sz], ic_type = 'ebic')
        model.fit(data.x, data.y)
        ic[sz] = model.ic_

    print("lowest point: ", np.argmin(ic))
    plt.plot(ic, 'o-')
    plt.xlabel('support_size')
    plt.ylabel('EBIC')
    plt.show()




.. image-sg:: /auto_gallery/4comimprove/images/sphx_glr_plot_golden_001.png
   :alt: plot golden
   :srcset: /auto_gallery/4comimprove/images/sphx_glr_plot_golden_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    lowest point:  5




.. GENERATED FROM PYTHON SOURCE LINES 29-34

Here the generated data contains 100 observations with 20 predictors, while 5 of them are useful (should be non-zero). The default information criterion is EBIC. From the figure, we can find that "support_size = 5" is the lowest point.

Compared with searching the optimal support size one by one from a candidate set with :math:`O(s_{max})` complexity, **golden-section** reduce the time complexity to :math:`O(ln(s_{max}))`, giving a significant computational improvement.

In `abess` package, this can be easily formed like:

.. GENERATED FROM PYTHON SOURCE LINES 34-42

.. code-block:: default




    model = LinearRegression(path_type = 'gs', s_min = 0, s_max = 20)
    model.fit(data.x, data.y)
    print("real coef:\n", np.nonzero(data.coef_)[0])
    print("predicted coef:\n", np.nonzero(model.coef_)[0])





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    real coef:
     [ 2  5 10 11 18]
    predicted coef:
     [ 2  5 10 11 18]




.. GENERATED FROM PYTHON SOURCE LINES 43-46

where `path_type = gs` means golden-section and `s_min`, `s_max` indicates the left and right bound of range of the support size. Note that in golden-section searching, we should not give `support_size`, which is only useful for sequential strategy.

The output of golden-section strategy suggests the optimal model size is accurately detected. Compare to the sequential searching, the golden section reduce the runtime because it skip some support sizes which are likely to be a non-optimal one:

.. GENERATED FROM PYTHON SOURCE LINES 46-61

.. code-block:: default




    from time import time

    t1 = time()
    model = LinearRegression(support_size = range(21))
    model.fit(data.x, data.y)
    print("sequential time: ", time() - t1)

    t2 = time()
    model = LinearRegression(path_type = 'gs', s_min = 0, s_max = 20)
    model.fit(data.x, data.y)
    print("golden-section time: ", time() - t2)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    sequential time:  0.0019943714141845703
    golden-section time:  0.0009970664978027344




.. GENERATED FROM PYTHON SOURCE LINES 62-63

The golden-section runs much faster than sequential method, espectially when the range of support size is large.

.. GENERATED FROM PYTHON SOURCE LINES 65-69

R tutorial
-------------

For R tutorial, please view [https://abess-team.github.io/abess/articles/v09-fasterSetting.html](https://abess-team.github.io/abess/articles/v09-fasterSetting.html).


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  0.070 seconds)


.. _sphx_glr_download_auto_gallery_4comimprove_plot_golden.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: plot_golden.py <plot_golden.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: plot_golden.ipynb <plot_golden.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
