{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Sparse matrix\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We sometimes meet with problems where the $N\u00d7p$ input matrix $X$ is extremely sparse, i.e., \nmany entries in $X$ have zero values. A notable example comes from document classification: aiming to assign classes to a document, making it easier to manage for publishers and news sites. The input variables for characterizing documents are generated from a so called \"bag-of-words\" model. In this model, each variable is scored for the presence of each of the words in the entire dictionary under consideration. Since most words are absent, the input variables for each document is mostly zero, and so the entire matrix is mostly zero. \n\nFor example, we create a sparse matrix like:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from scipy.sparse import coo_matrix\nimport numpy as np\n\nrow  = np.array([0, 1, 2, 3, 4, 4,  5, 6, 7, 7, 8, 9])\ncol  = np.array([0, 3, 1, 2, 4, 3, 5, 2, 3, 1, 5, 2])\ndata = np.array([4, 5, 7, 9, 1, 23, 4, 5, 6, 8, 77, 100])\nx = coo_matrix((data, (row, col)))\n\nprint(x.toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The sparse matrix can be directly used in `abess` pacakages. We just need to set argument `sparse_matrix = T`. Note that if the input matrix is not sparse matrix, the program would automatically transfer it into the sparse one, so this argument can also make some improvement.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from abess import LinearRegression\n\ncoef = np.array([1, 1, 1, 0, 0, 0])\ny = x.dot(coef)\nmodel = LinearRegression(sparse_matrix = True)\nmodel.fit(x, y)\n\nprint(\"real coef: \\n\", coef)\nprint(\"pred coef: \\n\", model.coef_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We compare the runtime when the input matrix is dense matrix:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from time import time\n\nt = time()\nmodel = LinearRegression()\nmodel.fit(x.toarray(), y)\nprint(\"dense matrix:  \", time() - t)\n\nt = time()\nmodel = LinearRegression(sparse_matrix = True)\nmodel.fit(x, y)\nprint(\"sparse matrix:  \", time() - t)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From the comparison, we see that the time required by sparse matrix is smaller, and this sould be more visible when the sparse imput matrix is large. Hence, we suggest to assign a sparse matrix to `abess` when the input matrix have a lot of zero entries.\n\n## R tutorial\n\nFor R tutorial, please view [https://abess-team.github.io/abess/articles/v09-fasterSetting.html](https://abess-team.github.io/abess/articles/v09-fasterSetting.html).\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}