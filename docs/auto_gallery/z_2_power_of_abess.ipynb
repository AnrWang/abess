{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Power of abess\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import abess\n# In this part, we are going to explore the power of the abess package\n# using simulated data. We compare the abess package with popular Python\n# packages:\n# `scikit-learn <https://scikit-learn.org/stable/supervised_learning.html#supervised-learning>`__\n# for linear and logistic regressions in the following section. Actually,\n# we also compare with\n# `python-glmnet <https://github.com/civisanalytics/python-glmnet>`__,\n# `statsmodels <https://github.com/statsmodels/statsmodels>`__ and\n# `L0bnb <https://github.com/alisaab/l0bnb>`__, but the python-glmnet\n# presents a poor prediction error, the statsmodels runs slow and the\n# L0bnb cannot adaptively choose sparsity level. So their results are not\n# showed here.\n\n\n# Simulation\n# ----------\n\n# Setting\n# ~~~~~~~\n\n# Both packages are compared in three aspects including the prediction\n# performance, the variable selection performance, and the computation\n# efficiency.\n\n# -  The prediction performance of the linear model is measured by\n#    :math:`||y\u2212\\hat{y}||_2` on a test set and for logistic regression\n#    this is measured by the area under the ROC Curve (AUC).\n# -  For the variable selection performance, we compute the coefficient\n#    error :math:`||\\beta - \\hat{\\beta}||_2`, true positive rate (TPR,\n#    which is the proportion of varibales in the active set that are\n#    correctly identified) and the false positive rate (FPR, which is the\n#    proportion of the varibales in the inactive set that are falsely\n#    identified as a signal).\n# -  Timings of the CPU execution are recorded in seconds and all the\n#    performances are averaged over 20 replications on a sequence of 100\n#    regularization parameters.\n\n# The simulated data are made by ``abess.datasets.make_glm_data()``. The\n# number of predictors is :math:`p=8000` and the size of data is\n# :math:`n=500`. The true coefficient contains :math:`k=10` nonzero\n# entries uniformly distributed in :math:`[b,B]`. For linear (gaussian)\n# data, we set :math:`b = 5\\sqrt{2\\ln p / n}` and :math:`B = 100b`. For\n# logistic (binomial) data, we set :math:`b = 10\\sqrt{2\\ln p / n}` and\n# :math:`B = 5b`. In each regression, we test for both low\n# (:math:`\\rho=0.1`) and high correlation (:math:`\\rho=0.7`) scenarios.\n# What\u2019s more, a random noise generated from a standard Gaussian\n# distribution is added to the linear predictor :math:`x\u2032\u03b2` for linear\n# regression.\n\n# All experiments are evaluated on a Ubuntu platform with Intel(R)\n# Core(TM) i9-9940X CPU @ 3.30GHz and 48 RAM.\n\n# .. code:: bash\n\n#    $ python abess/docs/simulation/Python/perform.py\n\n# Results\n# ~~~~~~~\n\n#    For linear regression, we compare three methods in the two packages:\n#    Lasso, OMP and abess. For logistic regression, we compare two\n#    methods: lasso and abess.\n\n# The results are presented in the following pictures. The first column is\n# the result of linear regression and the second one is of logistic\n# regression.\n\n# -  Firstly, among all of the methods implemented in different packages,\n#    the estimator obtained by the abess package shows both the best\n#    prediction performance and the best coefficient error.\n# -  Secondly, the estimator obtained by the abess package can reasonably\n#    control FPR in a low level while the TPR stays at 1. (Since all\n#    methods\u2019 TPR are 1, the figure is not plotted.)\n# -  Furthermore, our abess package is highly efficient compared with\n#    other packages, especially in the linear regression.\n\n# |image0|\n\n# |image1|\n\n# R performance\n# ~~~~~~~~~~~~~\n\n# For R performance, please view\n# https://abess-team.github.io/abess/articles/v11-power-of-abess.html.\n\n# .. |image0| image:: ../image/perform.png\n# .. |image1| image:: ../image/timings.png"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}