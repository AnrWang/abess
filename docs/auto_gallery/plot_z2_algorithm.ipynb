{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# ABESS algorithm: details\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction \n\nThe ABESS algorithm employing \"splicing\" technique can exactly solve\ngeneral best subset problem in a polynomial time. The aim of this page\nto provide a complete and coherent documentation for ABESS algorithm\nsuch that users can easily understand the ABESS algorithm and its\nvariants, thereby facilitating the usage of ``abess`` software.\n\n## linear regression \n\n\n### Sacrifices\n\nConsider the $\\ell_{0}$ constraint minimization problem,\n\n\\begin{align}\\min _{\\boldsymbol{\\beta}} \\mathcal{L}_{n}(\\beta), \\quad \\text { s.t }\\|\\boldsymbol{\\beta}\\|_{0} \\leq \\mathrm{s},\\end{align}\n\nwhere\n$\\mathcal{L}_{n}(\\boldsymbol \\beta)=\\frac{1}{2 n}\\|y-X \\beta\\|_{2}^{2} .$\nWithout loss of generality, we consider\n$\\|\\boldsymbol{\\beta}\\|_{0}=\\mathrm{s}$. Given any initial set\n$\\mathcal{A} \\subset \\mathcal{S}=\\{1,2, \\ldots, p\\}$ with\ncardinality $|\\mathcal{A}|=s$, denote\n$\\mathcal{I}=\\mathcal{A}^{\\mathrm{c}}$ and compute:\n\n\\begin{align}\\hat{\\boldsymbol{\\beta}}=\\arg \\min _{\\boldsymbol{\\beta}_{\\mathcal{I}}=0} \\mathcal{L}_{n}(\\boldsymbol{\\beta}).\\end{align}\n\nWe call $\\mathcal{A}$ and $\\mathcal{I}$ as the active set\nand the inactive set, respectively.\n\nGiven the active set $\\mathcal{A}$ and\n$\\hat{\\boldsymbol{\\beta}}$, we can define the following two types\nof sacrifices:\n\n1. Backward sacrifice: For any $j \\in \\mathcal{A}$, the magnitude\nof discarding variable $j$ is,\n\n\\begin{align}\\xi_{j}=\\mathcal{L}_{n}\\left(\\hat{\\boldsymbol{\\beta}}^{\\mathcal{A} \\backslash\\{j\\}}\\right)-\\mathcal{L}_{n}\\left(\\hat{\\boldsymbol{\\beta}}^{\\mathcal{A}}\\right)=\\frac{X_{j}^{\\top} X_{j}}{2 n}\\left(\\hat{\\boldsymbol\\beta}_{j}\\right)^{2},\\end{align}\n\n2. Forward sacrifice: For any $j \\in \\mathcal{I}$, the magnitude\nof adding variable $j$ is,\n\n\\begin{align}\\zeta_{j}=\\mathcal{L}_{n}\\left(\\hat{\\boldsymbol{\\beta}^{\\mathcal{A}}}\\right)-\\mathcal{L}_{n}\\left(\\hat{\\boldsymbol{\\beta}}^{\\mathcal{A}}+\\hat{t}^{\\{j\\}}\\right)=\\frac{X_{j}^{\\top} X_{j}}{2 n}\\left(\\frac{\\hat{\\boldsymbol d}_{j}}{X_{j}^{\\top} X_{j} / n}\\right)^{2}.\\end{align}\n\n| where\n  $\\hat{t}=\\arg \\min _{t} \\mathcal{L}_{n}\\left(\\hat{\\boldsymbol{\\beta}}^{\\mathcal{A}}+t^{\\{j\\}}\\right), \\hat{\\boldsymbol d}_{j}=X_{j}^{\\top}(y-X \\hat{\\boldsymbol{\\beta}}) / n$.\n  Intuitively, for $j \\in \\mathcal{A}$ (or\n  $j \\in \\mathcal{I}$ ), a large $\\xi_{j}$ (or\n  $\\zeta_{j}$) implies the $j$ th variable is potentially\n  important.\n\n\n### Algorithm\n\n\n#### Best-Subset Selection with a Given Support Size\n\nUnfortunately, it is noteworthy that these two sacrifices are\nincomparable because they have different sizes of support set. However,\nif we exchange some \"irrelevant\" variables in $\\mathcal{A}$ and\nsome \"important\" variables in $\\mathcal{I}$, it may result in a\nhigher-quality solution. This intuition motivates our splicing method.\nSpecifically, given any splicing size $k \\leq s$, define\n\n\\begin{align}\\mathcal{A}_{k}=\\left\\{j \\in \\mathcal{A}: \\sum_{i \\in \\mathcal{A}} \\mathrm{I}\\left(\\xi_{j} \\geq \\xi_{i}\\right) \\leq k\\right\\},\\end{align}\n\nto represent $k$ least relevant variables in $\\mathcal{A}$\nand,\n\n\\begin{align}\\mathcal{I}_{k}=\\left\\{j \\in \\mathcal{I}: \\sum_{i \\in \\mathcal{I}} \\mid\\left(\\zeta_{j} \\leq \\zeta_{i}\\right) \\leq k\\right\\},\\end{align}\n\nto represent $k$ most relevant variables in $\\mathcal{I} .$\n\n| Then, we splice $\\mathcal{A}$ and $\\mathcal{I}$ by\n  exchanging $\\mathcal{A}_{k}$ and $\\mathcal{I}_{k}$ and\n  obtain a new active\n  set:$\\tilde{\\mathcal{A}}=\\left(\\mathcal{A} \\backslash \\mathcal{A}_{k}\\right) \\cup \\mathcal{I}_{k}.$\n  Let\n  $\\tilde{\\mathcal{I}}=\\tilde{\\mathcal{A}}^{c}, \\tilde{\\boldsymbol{\\beta}}=\\arg \\min _{\\boldsymbol{\\beta}_{\\overline{\\mathcal{I}}=0}} \\mathcal{L}_{n}(\\boldsymbol{\\beta})$,\n  and $\\tau_{s}>0$ be a threshold. If $\\tau_{s}<\\mathcal{L}_{n}(\\hat{\\boldsymbol\\beta})-\\mathcal{L}_{n}(\\tilde{\\boldsymbol\\beta})$,\n  then $\\tilde{A}$ is preferable to $\\mathcal{A} .$ \n| The\n  active set can be updated\n  iteratively until the loss function cannot be improved by splicing.\n  Once the algorithm recovers the true active set, we may splice some\n  irrelevant variables, and then the loss function may decrease\n  slightly. The threshold $\\tau_{s}$ can reduce this unnecessary\n  calculation. Typically, $\\tau_{s}$ is relatively small, e.g.\n  $\\tau_{s}=0.01 s \\log (p) \\log (\\log n) / n.$\n\n\n##### Algorithm 1: BESS.Fix(s): Best-Subset Selection with a given support size $s$.\n\n1. Input: $X, y$, a positive integer $k_{\\max }$, and a\n   threshold $\\tau_{s}$.\n\n2. Initialize: \n\n\\begin{align}\\mathcal{A}^{0}=\\left\\{j: \\sum_{i=1}^{p} \\mathrm{I}\\left(\\left|\\frac{X_{j}^{\\top} y}{\\sqrt{X_{j}^{\\top} X_{j}}}\\right| \\leq \\left| \\frac{X_{i}^{\\top} y}{\\sqrt{X_{i}^{\\top} X_{i}}}\\right| \\leq \\mathrm{s}\\right\\}, \\mathcal{I}^{0}=\\left(\\mathcal{A}^{0}\\right)^{c}\\right.\\end{align}\n\nand $\\left(\\boldsymbol\\beta^{0}, d^{0}\\right):$\n\n\\begin{align}&\\boldsymbol{\\beta}_{\\mathcal{I}^{0}}^{0}=0,\\\\\n         &d_{\\mathcal{A}^{0}}^{0}=0,\\\\\n      &\\boldsymbol{\\beta}_{\\mathcal{A}^{0}}^{0}=\\left(\\boldsymbol{X}_{\\mathcal{A}^{0}}^{\\top} \\boldsymbol{X}_{\\mathcal{A}^{0}}\\right)^{-1} \\boldsymbol{X}_{\\mathcal{A}^{0}}^{\\top} \\boldsymbol{y},\\\\\n      &d_{\\mathcal{I}^{0}}^{0}=X_{\\mathcal{I}^{0}}^{\\top}\\left(\\boldsymbol{y}-\\boldsymbol{X} \\boldsymbol{\\beta}^{0}\\right).\\end{align}\n\n3. For $m=0,1, \\ldots$, do\n\n      .. math:: \\left(\\boldsymbol{\\beta}^{m+1}, \\boldsymbol{d}^{m+1}, \\mathcal{A}^{m+1}, \\mathcal{I}^{m+1}\\right)= \\text{Splicing} \\left(\\boldsymbol{\\beta}^{m}, \\boldsymbol{d}^{m}, \\mathcal{A}^{m}, \\mathcal{I}^{m}, k_{\\max }, \\tau_{s}\\right).\n\n      If $\\left(\\mathcal{A}^{m+1}, \\mathcal{I}^{m+1}\\right)=\\left(\\mathcal{A}^{m},\\mathcal{I}^{m}\\right)$,\n      then stop.\n\n   End For\n\n4. Output\n   $(\\hat{\\boldsymbol{\\beta}}, \\hat{\\boldsymbol{d}}, \\hat{\\mathcal{A}}, \\hat{\\mathcal{I}})=\\left(\\boldsymbol{\\beta}^{m+1}, \\boldsymbol{d}^{m+1} \\mathcal{A}^{m+1}, \\mathcal{I}^{m+1}\\right).$\n\n\n##### Algorithm 2: Splicing $\\left(\\boldsymbol\\beta, d, \\mathcal{A}, \\mathcal{I}, k_{\\max }, \\tau_{s}\\right)$\n\n1. Input:\n   $\\boldsymbol{\\beta}, \\boldsymbol{d}, \\mathcal{A}, \\mathcal{I}, k_{\\max }$,\n   and $\\tau_{\\mathrm{s}} .$\n\n2. Initialize: \n   $L_{0}=L=\\frac{1}{2 n}\\|y-X \\beta\\|_{2}^{2}$, and set\n\n   .. math:: \\xi_{j}=\\frac{X_{j}^{\\top} X_{j}}{2 n}\\left(\\beta_{j}\\right)^{2}, \\zeta_{j}=\\frac{X_{j}^{\\top} X_{j}}{2 n}\\left(\\frac{d_{j}}{X_{j}^{\\top} X_{j} / n}\\right)^{2}, j=1, \\ldots, p.\n\n3. For $k=1,2, \\ldots, k_{\\max }$, do\n\n      .. math::\n\n         \\mathcal{A}_{k}=\\left\\{j \\in \\mathcal{A}: \\sum_{i \\in \\mathcal{A}} \\mathrm{I}\\left(\\xi_{j} \\geq \\xi_{i}\\right) \\leq k\\right\\},\\\\\n         \\mathcal{I}_{k}=\\left\\{j \\in \\mathcal{I}: \\sum_{i \\in \\mathcal{I}} \\mathrm{I}\\left(\\zeta_{j} \\leq \\zeta_{i}\\right) \\leq k\\right\\}.\n\n      Let\n      $\\tilde{\\mathcal{A}}_{k}=\\left(\\mathcal{A} \\backslash \\mathcal{A}_{k}\\right) \\cup \\mathcal{I}_{k}, \\tilde{\\mathcal{I}}_{k}=\\left(\\mathcal{I} \\backslash \\mathcal{I}_{k}\\right) \\cup \\mathcal{A}_{k}$\n      and solve:\n\n      .. math::\n\n         \\tilde{\\boldsymbol{\\beta}}_{{\\mathcal{A}}_{k}}=\\left(\\boldsymbol{X}_{\\mathcal{A}_{k}}^{\\top} \\boldsymbol{X}_{{\\mathcal{A}}_{k}}\\right)^{-1} \\boldsymbol{X}_{{\\mathcal{A}_{k}}}^{\\top} y, \\quad \\tilde{\\boldsymbol{\\beta}}_{{\\mathcal{I}}_{k}}=0\\\\\n         \\tilde{\\boldsymbol d}_{\\mathcal{I}^k}=X_{\\mathcal{I}^k}^{\\top}(y-X \\tilde{\\beta}) / n,\\quad \\tilde{\\boldsymbol d}_{\\mathcal{A}^k} = 0.\n\n      Compute:\n      $\\mathcal{L}_{n}(\\tilde{\\boldsymbol\\beta})=\\frac{1}{2 n}\\|y-X \\tilde{\\boldsymbol\\beta}\\|_{2}^{2}.$\n      If $L>\\mathcal{L}_{n}(\\tilde{\\boldsymbol\\beta})$, then\n\n      .. math::\n\n         (\\hat{\\boldsymbol{\\beta}}, \\hat{\\boldsymbol{d}}, \\hat{\\mathcal{A}}, \\hat{\\mathcal{I}})=\\left(\\tilde{\\boldsymbol{\\beta}}, \\tilde{\\boldsymbol{d}}, \\tilde{\\mathcal{A}}_{k}, \\tilde{\\mathcal{I}}_{k}\\right)\\\\\n         L=\\mathcal{L}_{n}(\\tilde{\\boldsymbol\\beta}).\n\n   End for\n\n3. If $L_{0}-L<\\tau_{s}$, then\n   $(\\hat{\\boldsymbol\\beta}, \\hat{d}, \\hat{A}, \\hat{I})=(\\boldsymbol\\beta, d, \\mathcal{A}, \\mathcal{I}).$\n\n2. Output\n   $(\\hat{\\boldsymbol{\\beta}}, \\hat{\\boldsymbol{d}}, \\hat{\\mathcal{A}}, \\hat{\\mathcal{I}})$.\n\n#### Determining the Best Support Size with SIC\n\nIn practice, the support size is usually unknown. We use a datadriven\nprocedure to determine s. For any active set $\\mathcal{A}$, define\nan $\\mathrm{SIC}$ as follows:\n\n\\begin{align}\\operatorname{SIC}(\\mathcal{A})=n \\log \\mathcal{L}_{\\mathcal{A}}+|\\mathcal{A}| \\log (p) \\log \\log n,\\end{align}\n\nwhere\n$\\mathcal{L}_{\\mathcal{A}}=\\min _{\\beta_{\\mathcal{I}}=0} \\mathcal{L}_{n}(\\beta), \\mathcal{I}=(\\mathcal{A})^{c}$.\nTo identify the true model, the model complexity penalty is\n$\\log p$ and the slow diverging rate $\\log \\log n$ is set to\nprevent underfitting. Theorem 4 states that the following ABESS\nalgorithm selects the true support size via SIC.\n\nLet $s_{\\max }$ be the maximum support size. We suggest\n$s_{\\max }=o\\left(\\frac{n}{\\log p}\\right)$ as the maximum possible\nrecovery size. Typically, we set\n$s_{\\max }=\\left[\\frac{n}{\\log p \\log \\log n}\\right]$ where\n$[x]$ denotes the integer part of $x$.\n\n\n##### Algorithm 3: ABESS.\n\n1. Input: $X, y$, and the maximum support size $s_{\\max } .$\n\n2. For $s=1,2, \\ldots, s_{\\max }$, do\n\n   .. math:: \\left(\\hat{\\boldsymbol{\\beta}}_{s}, \\hat{\\boldsymbol{d}}_{s}, \\hat{\\mathcal{A}}_{s}, \\hat{\\mathcal{I}}_{s}\\right)= \\text{BESS.Fixed}(s).\n\n   End for\n\n3. Compute the minimum of SIC:\n\n   .. math:: s_{\\min }=\\arg \\min _{s} \\operatorname{SIC}\\left(\\hat{\\mathcal{A}}_{s}\\right).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# 4. Output\n#    :math:`\\left(\\hat{\\boldsymbol{\\beta}}_{s_{\\min}}, \\hat{\\boldsymbol{d}}_{s_{\\min }}, \\hat{A}_{s_{\\min }}, \\hat{\\mathcal{I}}_{s_{\\min }}\\right) .`\n\n# Group linear model\n# ------------------\n\n# .. _sacrifices-2:\n\n# Sacrifices\n# ~~~~~~~~~~\n\n# Consider the :math:`\\ell_{0,2}` constraint minimization problem with\n# :math:`n` samples and :math:`J` non-overlapping groups,\n\n# .. math:: \\min _{\\boldsymbol{{\\boldsymbol\\beta}}} \\mathcal{L}({\\boldsymbol\\beta}), \\quad \\text { s.t }\\|{{\\boldsymbol\\beta}}\\|_{0,2} \\leq \\mathrm{T}.\n\n# where :math:`\\mathcal{L}({\\boldsymbol\\beta})` is the negative\n# log-likelihood function and support size :math:`\\mathrm{T}` is a\n# positive number. Without loss of generality, we consider\n# :math:`\\|\\boldsymbol{{\\boldsymbol\\beta}}\\|_{0,2}=\\mathrm{T}`. Given any\n# group subset :math:`\\mathcal{A} \\subset \\mathcal{S}=\\{1,2, \\ldots, J\\}`\n# with cardinality :math:`|\\mathcal{A}|=\\mathrm{T}`, denote\n# :math:`\\mathcal{I}=\\mathcal{A}^{\\mathrm{c}}` and compute:\n\n# .. math:: \\hat{{{\\boldsymbol\\beta}}}=\\arg \\min _{{{\\boldsymbol\\beta}}_{\\mathcal{I}}=0} \\mathcal{L}({{\\boldsymbol\\beta}}).\n\n# | We call :math:`\\mathcal{A}` and :math:`\\mathcal{I}` as the selected\n#   group subset and the unselected group subset, respectively.\n# | Denote\n#   :math:`g_{G_j} = [{\\nabla} \\mathcal{L}({\\boldsymbol\\beta})]_{G_j} ` as\n#   the :math:`j`\\ th group gradient of :math:`({\\boldsymbol\\beta})` and\n#   :math:`h_{G_j} = [{\\nabla}^2 \\mathcal{L}({\\boldsymbol\\beta})]_{G_j} `\n#   as the :math:`j`\\ th group diagonal sub-matrix of hessian matrix of\n#   :math:`\\mathcal{L}({\\boldsymbol\\beta})`. Let dual variable\n#   :math:`d_{G_j} = -g_{G_j}` and\n#   :math:`\\Psi_{G_j} =  (h_{G_j})^{\\frac{1}{2}}`.\n\n# Given the selected group subset :math:`\\mathcal{A}` and\n# :math:`\\hat{\\boldsymbol{{\\boldsymbol\\beta}}}`, we can define the\n# following two types of sacrifices:\n\n# 1. Backward sacrifice: For any :math:`j \\in \\mathcal{A}`, the magnitude\n#    of discarding group :math:`j` is,\n\n#    .. math:: \\xi_j = \\mathcal{L}({\\boldsymbol\\beta}^{\\mathcal{A}^k\\backslash j})-\\mathcal{L}({\\boldsymbol\\beta}^k)=\\frac{1}{2}({\\boldsymbol\\beta}^k_{G_j})^k h^k_{G_j}{\\boldsymbol\\beta}^k_{G_j} = \\frac{1}{2}\\|\\bar{{\\boldsymbol\\beta}}_{G_j}^k\\|_2^2,\n\n#    where :math:`{\\boldsymbol\\beta}^{\\mathcal{A}^k\\backslash j}` is the\n#    estimator assigning the :math:`j`\\ th group of\n#    :math:`{\\boldsymbol\\beta}^k` to be zero and\n#    :math:`\\bar {\\boldsymbol\\beta}_{G_j}^k=\\Psi^k_{G_j} {\\boldsymbol\\beta}_{G_j}^k`.\n\n# 2. Forward sacrifice: For any :math:`j \\in \\mathcal{I}`, the magnitude\n#    of adding variable :math:`j` is,\n\n#    .. math:: \\zeta_{j}=\\mathcal{L}({\\boldsymbol\\beta}^k)-\\mathcal{L}({\\boldsymbol\\beta}^k+t_j^k)=\\frac{1}{2}(d_{G_j}^k)^\\top (h^k_{G_j})^{-1} d^k_{G_j}= \\frac{1}{2}\\|\\bar{d}^k_{G_j}\\|_2^2,\n\n#    where\n#    :math:`t^k_j = \\arg\\min\\limits_{t_{G_j} \\neq 0}L({\\boldsymbol\\beta}^k+t)`\n#    and :math:`\\bar d_{G_j}^k = (\\Psi^k_{G_j})^{-1} d^k_{G_j}`.\n\n# Intuitively, for :math:`j \\in \\mathcal{A}` (or :math:`j \\in \\mathcal{I}`\n# ), a large :math:`\\xi_{j}` (or :math:`\\zeta_{j}`) implies the :math:`j`\n# th group is potentially important.\n\n# We show four useful examples in the following.\n\n# .. _case-1--group-linear-model:\n\n# Case 1 : Group linear model.\n# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n# In group linear model, the loss function is\n\n# .. math::\n\n#    \\begin{equation*}\n#    \\mathcal{L}({\\boldsymbol\\beta}) = \\frac{1}{2}\\|y-X{\\boldsymbol\\beta}\\|_2^2.\n#    \\end{equation*}\n\n# We have\n\n# .. math::\n\n#    \\begin{equation*}\n#    d_{G_j} = X_{G_j}^\\top(y-X{\\boldsymbol\\beta})/n,\\ \\Psi_{G_j} = (X_{G_j}^\\top X_{G_j}/n)^{\\frac{1}{2}}, \\ j=1,\\ldots,J.\n#    \\end{equation*}\n\n# Under the assumption of orthonormalization, that is\n# :math:`X_{G_j}^\\top X_{G_j}/n = I_{p_j}, j=1,\\ldots, J`. we have\n# :math:`\\Psi_{G_j}=I_{p_j}`. Thus for linear regression model, we do not\n# need to update :math:`\\Psi` during iteration procedures.\n\n# .. _case-2--group-logistic-model:\n\n# Case 2 : Group logistic model.\n# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n# Given the data :math:`\\{(X_i, y_i)\\}_{i=1}^{n}` with\n# :math:`y_i \\in \\{0, 1\\}, X_i \\in \\mathbb{R}^p`, and denote\n# :math:`X_i = (X_{i, G_1}^\\top,\\ldots, X_{i, G_J}^\\top)^\\top`.\n\n# Consider the logistic model\n# :math:`\\log\\{\\pi/(1-\\pi)\\} = {\\boldsymbol\\beta}_0 +  x^\\top{\\boldsymbol\\beta}`\n# with :math:`x \\in \\mathbb{R}^p` and :math:`\\pi = P(y=1|x)`.\n\n# Thus the negative log-likelihood function is:\n\n# .. math::\n\n#    \\begin{equation*}\n#    \\mathcal{L}({\\boldsymbol\\beta}_0, {\\boldsymbol\\beta}) =  \\sum_{i=1}^n  \\{\\log(1+\\exp({\\boldsymbol\\beta}_0+X_i^\\top {\\boldsymbol\\beta}))-y_i ({\\boldsymbol\\beta}_0+X_i^\\top {\\boldsymbol\\beta})\\}.\n#    \\end{equation*}\n\n# We have\n\n# .. math::\n\n#    \\begin{equation*}\n#    d_{G_j} = X_{G_j}^\\top(y-\\pi),\\ \\Psi_{G_j} = (X_{G_j}^\\top W X_{G_j})^{\\frac{1}{2}}, \\ j=1,\\ldots,J,\n#    \\end{equation*}\n\n# where :math:`\\pi = (\\pi_1,\\ldots,\\pi_n)` with\n# :math:`\\pi_i = \\exp(X_i^\\top {\\boldsymbol\\beta})/(1+\\exp(X_i^\\top {\\boldsymbol\\beta}))`,\n# and :math:`W` is a diagonal matrix with :math:`i`\\ th diagonal entry\n# equal to :math:`\\pi_i(1-\\pi_i)`.\n\n# .. _case-3--group-poisson-model:\n\n# Case 3 : Group poisson model.\n# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n# Given the data :math:`\\{(X_i, y_i)\\}_{i=1}^{n}` with\n# :math:`y_i \\in \\mathbb{N}, X_i \\in \\mathbb{R}^p`, and denote\n# :math:`X_i = (X_{i, G_1}^\\top,\\ldots, X_{i, G_J}^\\top)^\\top`.\n\n# Consider the poisson model\n# :math:`\\log(\\mathbb{E}(y|x)) = {\\boldsymbol\\beta}_0 + x^\\top {\\boldsymbol\\beta}`\n# with :math:`x \\in \\mathbb{R}^p`.\n\n# Thus the negative log-likelihood function is:\n\n# .. math::\n\n#    \\begin{equation*}\n#      \\mathcal{L}({\\boldsymbol\\beta}_0, {\\boldsymbol\\beta}) =  \\sum_{i=1}^n  \\{\\exp({\\boldsymbol\\beta}_0+X_i^\\top {\\boldsymbol\\beta})+\\log(y_i !)-y_i ({\\boldsymbol\\beta}_0+X_i^\\top {\\boldsymbol\\beta})\\}.\n#    \\end{equation*}\n\n# We have:\n\n# .. math::\n\n#    \\begin{equation*}\n#    d_{G_j} = X_{G_j}^\\top(y-\\eta),\\ \\Psi_{G_j} = (X_{G_j}^\\top W X_{G_j})^{\\frac{1}{2}}, \\ j=1,\\ldots,J,\n#    \\end{equation*}\n\n# where :math:`\\eta = (\\eta_1,\\ldots,\\eta_n)` with\n# :math:`\\eta_i = \\exp({\\boldsymbol\\beta}_0+X_i^\\top{\\boldsymbol\\beta})`,\n# and :math:`W` is a diagonal matrix with :math:`i`\\ th diagonal entry\n# equal to :math:`\\eta_i`.\n\n# .. _case-4--group-cox-proportional-hazard-model:\n\n# Case 4 : Group Cox proportional hazard model.\n# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n# Given the survival data :math:`\\{(T_i, \\delta_i, x_i)\\}_{i=1}^n` with\n# observation of survival time :math:`T_i` an censoring indicator\n# :math:`\\delta_i`.\n\n# Consider the Cox proportional hazard model\n# :math:`\\lambda(x|t) = \\lambda_0(t) \\exp(x^\\top {\\boldsymbol\\beta})`\n# with a baseline hazard :math:`\\lambda_0(t)` and\n# :math:`x \\in \\mathbb{R}^p`. By the method of partial likelihood,\n# we can write the negative log-likelihood function as:\n\n# .. math::\n\n#    \\begin{equation*}\n#      \\mathcal{L}({\\boldsymbol\\beta}) =  \\log\\{\\sum_{i':T_{i'} \\geqslant T_i} \\exp(X_i^\\top{\\boldsymbol\\beta})\\}-\\sum_{i:\\delta_i = 1} X_i^\\top {\\boldsymbol\\beta}.\n#    \\end{equation*}\n\n# We have:\n\n# .. math::\n\n#    \\begin{align*}\n#      &d_{G_j} = \\sum_{i:\\delta_i=1} (X_{i, G_j} - \\sum_{i':T_{i'} > T_i} X_{i', G_j} \\omega_{i, i'}),\\\\\n#      &\\Psi_{G_j}=\\{\\sum_{i:\\delta_i=1} (\\{\\sum_{i':T_{i'} > T_i} \\omega_{i, i'} X_{i',G_j}\\}\\{\\sum_{i':T_{i'} > T_i} \\omega_{i, i'} X_{i',G_j}\\}^\\top-\\sum_{i':T_{i'} > T_i} \\omega_{i, i'} X_{i',G_j} X_{i', G_j}^\\top)\\}^{\\frac{1}{2}},\n#    \\end{align*}\n\n# where\n# :math:`\\omega_{i, i'} = \\exp(X_{i'}^\\top{\\boldsymbol\\beta})/\\sum_{i':T_{i'} > T_i} \\exp(X_{i'}^\\top {\\boldsymbol\\beta})`.\n\n# .. _algorithm-2:\n\n# Algorithm\n# ~~~~~~~~~\n\n# Best Group Subset Selection with a determined support size\n# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n# Motivated by the definition of sacrifices, we can extract the\n# \"irrelevant\" groups in :math:`\\mathcal{A}` and the \"important\" groups in\n# :math:`\\mathcal{I}`, respectively, and then exchange them to get a\n# high-quality solution.\n\n# Given any exchange subset size :math:`C \\leq C_{max}`, define the\n# exchanged group subset as:\n\n# .. math:: \\mathcal{S}_{C,1}=\\left\\{j \\in \\mathcal{A}: \\sum_{i \\in \\mathcal{A}} \\mathrm{I}\\left(\\frac{1}{p_j}\\xi_{j} \\geq \\frac{1}{p_i}\\xi_{i}\\right) \\leq C\\right\\},\n\n# and\n\n# .. math:: \\mathcal{S}_{C,2}=\\left\\{j \\in \\mathcal{I}: \\sum_{i \\in \\mathcal{I}} I\\left(\\frac{1}{p_j}\\zeta_{j} \\leq \\frac{1}{p_i}\\zeta_{i}\\right) \\leq C\\right\\},\n\n# where :math:`p_j` is the number of variables in :math:`j`\\ th group.\n\n# From the definition of sacrifices,\n# :math:`\\mathcal{S}_{C,1}\\ (\\mathcal{S}_{C,2})` can be interpreted as the\n# groups in :math:`\\mathcal{A}\\ (\\mathcal{I})` with :math:`C` smallest\n# (largest) contributions to the loss function. Then, we splice\n# :math:`\\mathcal{A}` and :math:`\\mathcal{I}` by exchanging\n# :math:`\\mathcal{S}_{C,1}` and :math:`\\mathcal{S}_{C,2}` and obtain a\n# novel selected group subset\n\n# .. math:: \\tilde{\\mathcal{A}}=\\left(\\mathcal{A} \\backslash \\mathcal{S}_{C,1}\\right) \\cup \\mathcal{S}_{C,2}.\n\n# Let\n# :math:`\\tilde{\\mathcal{I}}=\\tilde{\\mathcal{A}}^{c}, \\tilde{\\boldsymbol{{\\boldsymbol\\beta}}}=\\arg \\min _{\\boldsymbol{{\\boldsymbol\\beta}}_{\\overline{\\mathcal{I}}}=0} \\mathcal{L}(\\boldsymbol{{\\boldsymbol\\beta}})`,\n# and :math:`\\pi_{T}>0` be a threshold to eliminate unnecessary\n# iterations.\n\n# We summarize the group-splicing algorithm as follows:\n\n# .. _algorithm-1-group-splicing:\n\n# Algorithm 1: Group-Splicing.\n# ''''''''''''''''''''''''''''\n\n# 1. Input:\n#    :math:`X,\\ y,\\ \\{G_j\\}_{j=1}^J,\\ T, \\ \\mathcal{A}^0,\\ \\pi_T, \\ C_{\\max}`.\n\n# 2. Initialize :math:`k=0` and solve primal variable :math:`{\\boldsymbol\\beta}^{k}` and dual variable :math:`d^{k}:`\n\n#    .. math::\n\n#       \\begin{align*}\n#          &{{\\boldsymbol\\beta}}_{\\mathcal{A}^{k}}^{k}=[\\arg \\min _{{{\\boldsymbol\\beta}}_{\\mathcal{I}^{k}}=0} \\mathcal{L}({{\\boldsymbol\\beta}})]_{\\mathcal{A}^{k}},\\ {{\\boldsymbol\\beta}}_{\\mathcal{I}^{k}}^{k}=0,\\\\\n#          &d_{\\mathcal{I}^{k}}^{k}=[\\nabla \\mathcal{L}({\\boldsymbol\\beta}^k)]_{\\mathcal{I}^k},\\ d_{\\mathcal{A}^{k}}^{k}=0.\\\\\n#          \\end{align*}\n\n# 3. While :math:`\\mathcal{A}^{k+1} \\neq \\mathcal{A}^{k}`, do\n\n#       Compute :math:`L=\\mathcal{L}({\\boldsymbol\\beta}^k)` and :math:`( {\\bar{\\boldsymbol\\beta}}, {\\bar{d}} )`.\n      \n#       Update :math:`\\mathcal{S}_1^k, \\mathcal{S}_2^k`\n\n#       .. math::\n\n#          \\begin{align*}\n#          &\\mathcal{S}_1^k = \\{j \\in \\mathcal{A}^k: \\sum\\limits_{i\\in \\mathcal{A}^k} I(\\frac{1}{p_j}\\|{\\bar {\\boldsymbol\\beta}_{G_j}^k}\\|_2^2 \\geq \\frac{1}{p_i}\\|{\\bar {\\boldsymbol\\beta}_{G_i}^k}\\|_2^2) \\leq C_{\\max}\\},\\\\\n#          &\\mathcal{S}_2^k = \\{j \\in \\mathcal{I}^k: \\sum\\limits_{i\\in \\mathcal{I}^k} I(\\frac{1}{p_j}\\|{\\bar d_{G_j}^k}\\|_2^2 \\leq \\frac{1}{p_i}\\|{\\bar d_{G_i}^k}\\|_2^2) \\leq C_{\\max}\\}.\n#          \\end{align*}\n\n# 4. For :math:`C=C_{\\max}, \\ldots, 1`, do\n\n#       Let\n#       :math:`\\tilde{\\mathcal{A}}^k_C=(\\mathcal{A}^k\\backslash \\mathcal{S}_1^k)\\cup \\mathcal{S}_2^k\\ \\text{and}\\ \\tilde{\\mathcal{I}}^k_C = (\\mathcal{I}^k\\backslash \\mathcal{S}_2^k)\\cup \\mathcal{S}_1^k`.\n\n#       Update primal variable :math:`\\tilde{{\\boldsymbol\\beta}}` and dual\n#       variable :math:`\\tilde{d}`\n\n#       .. math::\n\n#          \\begin{align*}\n#          \\tilde{\\boldsymbol\\beta}=\\arg \\min _{{{\\boldsymbol\\beta}}_{\\tilde{\\mathcal{I}}^k_C}=0} \\mathcal{L}({{\\boldsymbol\\beta}}),\\ \\tilde d = \\nabla \\mathcal{L}(\\tilde{\\boldsymbol\\beta}).\n#          \\end{align*}\n\n#       Compute :math:`\\tilde L = \\mathcal{L}(\\tilde {\\boldsymbol\\beta})`.\n\n#       If :math:`L-\\tilde L < \\pi_T`, denote\n#       :math:`(\\tilde{\\mathcal{A}}^k_C, \\tilde{\\mathcal{I}}^k_C, \\tilde {\\boldsymbol\\beta} , \\tilde d )`\n#       as\n#       :math:`(\\mathcal{A}^{k+1}, \\mathcal{I}^{k+1}, {\\boldsymbol\\beta}^{k+1}, d^{k+1})`\n#       and break.\n\n#       Else, Update :math:`\\mathcal{S}_1^k \\text{ and } \\mathcal{S}_2^k`:\n\n#       .. math::\n\n#          \\begin{align*}\n#          &\\mathcal{S}_1^k = \\mathcal{S}_1^k\\backslash \\arg\\max\\limits_{i \\in \\mathcal{S}_1^k} \\{\\frac{1}{p_i}\\|{\\bar {\\boldsymbol\\beta}_{G_i}^k}\\|_2^2\\},\\\\\n#          &\\mathcal{S}_2^k = \\mathcal{S}_2^k\\backslash \\arg\\min\\limits_{i \\in \\mathcal{S}_2^k} \\{\\frac{1}{p_i}\\|{\\bar d_{G_i}^k}\\|_2^2\\}.\n#          \\end{align*}\n\n#    End For\n\n#       If\n#       :math:`\\left(\\mathcal{A}^{k+1}, \\mathcal{I}^{k+1}\\right)=\\left(\\mathcal{A}^{k}, \\mathcal{I}^{k}\\right)`,\n#       then stop.\n\n#    End While\n\n# 5. Output\n#    :math:`(\\hat{\\boldsymbol{{\\boldsymbol\\beta}}}, \\hat{\\boldsymbol{d}}, \\hat{\\mathcal{A}}, \\hat{\\mathcal{I}})=\\left(\\boldsymbol{{\\boldsymbol\\beta}}^{m+1}, \\boldsymbol{d}^{m+1} \\mathcal{A}^{m+1}, \\mathcal{I}^{m+1}\\right).`\n\n# Determining the best support size with information criterion\n# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n# | Practically, the optimal support size is usually unknown. Thus, we use\n#   a data-driven procedure to determine :math:`\\mathrm{T}`. Due to the\n#   computational burden of cross validation, we prefer information\n#   criterion to conduct the selection procedure.\n# | For any selected group subset :math:`\\mathcal{A}`, define an group\n#   information criterion(GIC) as follows:\n\n# .. math:: \\operatorname{GIC}(\\mathcal{A})=n \\log \\mathcal{L}_{\\mathcal{A}}+ \\log J \\log \\log n \\#\\{\\mathcal{A}\\},\n\n# | where\n#   :math:`\\mathcal{L}_{\\mathcal{A}}=\\min _{{\\boldsymbol\\beta}_{\\mathcal{I}}=0} \\mathcal{L}_{n}({\\boldsymbol\\beta}), \\mathcal{I}=(\\mathcal{A})^{c}` and\n#   :math:`\\#\\{\\mathcal{A}\\}` is the number of variables contained in :math:`\\cup_{j\\in \\mathcal{A}}G_j`.\n#   To identify the true model, the\n#   model complexity penalty is :math:`\\log J` and the slow diverging rate\n#   :math:`\\log \\log n` is set to prevent underfitting. Besides, we define\n#   the Bayesian group information criterion (BGIC) as follows:\n\n# .. math:: \\operatorname{BGIC}(\\mathcal{A})=n \\log \\mathcal{L}_{\\mathcal{A}}+ (\\gamma \\log J +\\log n)\\#\\{\\mathcal{A}\\},\n\n# where :math:`\\gamma` is a pre-determined positive constant, controlling\n# the diverging rate of group numbers :math:`J`.\n\n# | A natural idea to determine the optimal support size is regarding\n#   :math:`\\mathrm{T}` as a tuning parameter, and running GSplicing\n#   algorithm over a sequence about :math:`\\mathrm{T}`. Next, combined\n#   with aforementioned information criterion, we can obtain an optimal\n#   support size.\n# | Let :math:`T_{\\max }` be the maximum support size. We suggest\n#   :math:`T_{\\max }=o\\left(\\frac{n}{p_{\\max}\\log J}\\right)` where\n#   :math:`p_{\\max} = \\max_{j\\in \\mathcal{S}} p_j`.\n\n# We summarize the sequential group-splicing algorithm with GIC as\n# follows:\n\n# .. _algorithm-2-sequential-group-splicing-sgsplicing:\n\n# Algorithm 2: Sequential Group-Splicing (SGSplicing).\n# ''''''''''''''''''''''''''''''''''''''''''''''''''''\n\n# 1. Input:\n#    :math:`X,\\ y,\\ \\{G_j\\}_{j=1}^J,\\ T_{\\max}, \\ \\pi_T, \\ C_{\\max}.`\n\n# 2. For :math:`T=1,2, \\ldots, T_{\\max }`, do\n\n#    .. math:: \\left(\\hat{\\boldsymbol{{\\boldsymbol\\beta}}}_{T}, \\hat{\\boldsymbol{d}}_{T}, \\hat{\\mathcal{A}}_{T}, \\hat{\\mathcal{I}}_{T}\\right)=\\text{GSplicing}(X, y, \\{G_j\\}_{j=1}^J, T,  \\mathcal{A}^0_T, \\pi_T, C_{\\max}).\n\n#    End for\n\n# 3. Compute the minimum of GIC:\n\n#    .. math:: T_{\\min }=\\arg \\min _{T} \\operatorname{GIC}\\left(\\hat{\\mathcal{A}}_{T}\\right).\n\n# 4. Output\n#    :math:`\\left(\\hat{\\boldsymbol{{\\boldsymbol\\beta}}}_{T_{\\operatorname{min}}}, \\hat{\\boldsymbol{d}}_{T_{\\min }}, \\hat{\\mathcal{A}}_{T_{\\min }}, \\hat{\\mathcal{I}}_{T_{\\min }}\\right) .`\n\n# Nuisance selection \n# ------------------\n\n# Principal Component Analysis\n# ----------------------------\n\n# .. _sacrifices-3:\n\n# Sacrifices \n# ~~~~~~~~~~\n\n# Consider the :math:`\\ell_{0}` constraint minimization problem,\n\n# .. math::\n\n#    \\min_v\\ -v^T\\Sigma v,\\\\\n#    s.t.\\quad v^Tv = 1,\\ ||v||_0 = s,\n\n# where :math:`\\Sigma` is the given covariance matrix and :math:`s` is the\n# chosen sparsity level.\n\n# Denote the active set and inactive set as:\n\n# .. math::\n\n#    \\mathcal{A} = \\{i|v_i\\neq 0\\},\\quad\n#    \\mathcal{I} = \\{i|v_i = 0\\},\n\n# and :math:`\\alpha = -2\\Sigma v + 2\\beta v`. Since there are only\n# :math:`s` elements in :math:`\\mathcal{A}`, the definition can actually\n# be proved as:\n\n# .. math::\n\n#    \\mathcal{A} = \\{i|\\sum_j \n#    \tI(|v_i - \\frac{\\alpha_i}{\\rho}|\\leq|v_j - \\frac{\\alpha_j}{\\rho}|)\\leq s\\},\\\\\n#    \\mathcal{I} = \\{i|\\sum_j\n#    \tI(|v_i - \\frac{\\alpha_i}{\\rho}|\\leq|v_j - \\frac{\\alpha_j}{\\rho}|)> s\\},\\\\\n\n# where :math:`\\rho` is a constant and it decides the distribution in\n# :math:`\\mathcal{A}, \\mathcal{I}`. Now the choice of active and inactive\n# set is based on :math:`\\frac{\\alpha_i}{\\rho}`. When we change\n# :math:`\\rho`, we are actually exchanging the elements between\n# :math:`\\mathcal{A}` and :math:`\\mathcal{I}`. This exchanging is regular:\n# smaller :math:`|v_i-\\frac{\\alpha_i}{\\rho}|` is tend to be inactive and\n# larger is tend to be active.\n\n# Note that we can define forward and backward sacrifice here,\n\n# 1. Forward sacrifice: for each :math:`i\\in \\mathcal{I}`, the larger\n#    :math:`|v_i - \\frac{\\alpha_i}{\\rho}|`, the more possible to be\n#    exchanged to :math:`\\mathcal{A}`. Since :math:`v_i = 0`, we can focus\n#    on :math:`|\\alpha_i|`,\n\n#    .. math:: \\zeta_{i} = |\\alpha_i|.\n\n# 2. Backward sacrifice: for each :math:`i\\in \\mathcal{A}`, the smaller\n#    :math:`|v_i - \\frac{\\alpha_i}{\\rho}|`, the more possible to be\n#    exchanged to :math:`\\mathcal{I}`. Since\n#    :math:`v_i = H_{\\frac{2\\mu}{\\rho}}(v_i-\\frac{\\alpha_i}{\\rho})` and so\n#    that :math:`\\alpha_i=0`, we can only focus on :math:`|v_i|`,\n\n#    .. math:: \\xi_i = |v_i|.\n\n# .. _algorithm-3:\n\n# Algorithm\n# ~~~~~~~~~\n\n# .. _best-subset-selection-with-a-given-support-size-2:\n\n# Best-Subset Selection with a Given Support Size\n# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n# As we discuss above, we can iteratively solve :math:`v`, and in each\n# iteration, we compute:\n\n# .. math:: \\alpha = -2\\Sigma v + 2\\beta v,\n\n# and the active/inactive set. Then the sacrifices are:\n\n# .. math::\n\n#    \\begin{cases}\n#        \\zeta_{i} = |\\alpha_i|, & i\\in \\mathcal{I}\\\\\n#        \\xi_i = |v_i|, & i\\in \\mathcal{A}\n#    \\end{cases},\n\n# We try all number of the exchanging from 0 to :math:`\\min(s, p-s)` and\n# choose the best one with higher :math:`v^T\\Sigma v`. If no element need\n# to be exchanged, the program will return :math:`v` as the result.\n\n# Algorithm 1: SPCA\n# '''''''''''''''''\n\n# 1. Input :math:`s, \\Sigma` (or :math:`X`). If :math:`X` is given, set\n#    :math:`\\Sigma = cov(X)`;\n\n# 2. Initialize :math:`v` with :math:`s` non-zero positions;\n\n# 3. For :math:`m = 0, 1, \\cdots` do:\n\n#       Compute :math:`\\mathcal{A}`, :math:`\\mathcal{I}` and :math:`\\alpha`;\n\n#       Set :math:`v = \\text{Splicing}(s,\\Sigma, \\mathcal{A}, \\mathcal{I}, \\alpha)`;\n\n#       If :math:`v` is not changed, break.\n   \n#    End For\n\n# 4. Return :math:`v`.\n\n# .. _algorithm-2-splicing-2:\n\n# Algorithm 2: Splicing\n# '''''''''''''''''''''\n\n# 1. Input :math:`s,\\Sigma, \\mathcal{A}, \\mathcal{I}, \\alpha`;\n\n# 2. Compute forward sacrifices:\n#    :math:`\\zeta_{i} = |\\alpha_i|, i\\in \\mathcal{I}` and backward\n#    sacrifices: :math:`\\xi_i = |v_i|, i\\in \\mathcal{A}`;\n\n# 3. For :math:`k = 0, 1, \\cdots, \\min(s, p-s)` do:\n\n#       Exchange :math:`k` elements in :math:`\\mathcal{I}` with :math:`k`\n#       largest :math:`\\zeta` and in :math:`\\mathcal{A}` with :math:`k`\n#       smallest :math:`\\xi`;\n\n#       Form a normal PCA on active set to get :math:`v`;\n\n#       Re-compute :math:`v^T\\Sigma v`;\n\n#       Record the :math:`v_0 = \\arg\\max_v v^T\\Sigma v`;\n   \n#    End For\n\n# 4. Return :math:`v_0`.\n\n# Multiple SPCA\n# ^^^^^^^^^^^^^\n\n# Sometimes we require more than one principle components. Actually, we\n# can iteratively solve the largest principal component and then mapping\n# the covariance matrix to its orthogonal space:\n\n# .. math:: \\Sigma' = (1-vv^T)\\Sigma(1-vv^T),\n\n# where :math:`\\Sigma` is the currect covariance matrix and :math:`v` is\n# its (sparse) principal component solved above. We map it into\n# :math:`\u03a3^\u2032`, which indicates the orthogonal space of :math:`v`, and then\n# solve again.\n\n# Algorithm 3: Multi-SPCA \n# '''''''''''''''''''''''\n\n# 1. Input :math:`s, \\Sigma` (or :math:`X`), and :math:`number`. If\n#    :math:`X` is given, set :math:`\\Sigma = cov(X)`;\n\n# 2. For :math:`num = 1, 2, \\cdots, number`:\n\n#       Compute :math:`v = \\text{SPCA}(s,\\Sigma);`\n\n#       Set :math:`\\Sigma = (1-vv^T)\\Sigma(1-vv^T);`\n\n#       Record :math:`v;`\n\n#    End For\n\n# 3. Print all :math:`v`'s.\n\n# Group Principal Component Analysis\n# ----------------------------------\n\n# .. _sacrifices-4:\n\n# Sacrifices\n# ~~~~~~~~~~\n\n# With group information, consider the :math:`\\ell_{0}` constraint\n# minimization problem,\n\n# .. math::\n\n#    \\min_v\\ -v^T\\Sigma v,\\\\\n#    s.t.\\quad v^Tv = 1,\\ ||v||_{0,g} = s,\n\n# where :math:`\\Sigma ` is the given covariance matrix and :math:`s` is\n# the chosen sparsity level. :math:`||v||_{0,g}` indicates the number of\n# non-zero groups in :math:`v`, i.e.\n\n# .. math:: ||v||_{0,g} = \\sum_g I(||v_{(g)}||\\neq 0),\n\n# where :math:`v_{(g)}` is the :math:`g`-th group of predictors and\n# :math:`v^T = (v_{(1)}^T, v_{(2)}^T, \\cdots, v_{(G)}^T)`.\n\n# Similar to the `Principal Component\n# Analysis <#principal-component-analysis>`__, the problem can be\n# rewritten as:\n\n# .. math::\n\n#    \\mathcal{A} = \\{i|\\sum_j \n#    \tI(||v_i - \\frac{\\alpha_i}{\\rho}||_2\\leq||v_j - \\frac{\\alpha_j}{\\rho}||_2)\\leq s\\},\\\\\n#    \\mathcal{I} = \\{i|\\sum_j\n#    \tI(||v_i - \\frac{\\alpha_i}{\\rho}||_2\\leq||v_j - \\frac{\\alpha_j}{\\rho}||_2)> s\\},\\\\\n#\n# We can define forward and backward sacrifice by\n\n# 1. Forward sacrifice: for each :math:`i\\in \\mathcal{I}`, the larger\n#    :math:`||v_{(i)} - \\frac{\\alpha_{(i)}}{\\rho}||_2`, the more possible\n#    to be exchanged to :math:`\\mathcal{A}`. Since :math:`v_i = 0`, we can\n#    focus on :math:`||\\alpha_{(i)}||_2`,\n\n#    .. math:: \\zeta_{i} = ||\\alpha_{(i)}||_2.\n\n# 2. Backward sacrifice: for each :math:`i\\in \\mathcal{A}`, the smaller\n#    :math:`||v_{(i)} - \\frac{\\alpha_{(i)}}{\\rho}||_2`, the more possible\n#    to be exchanged to :math:`\\mathcal{I}`. Since\n#    :math:`v_i = H_{\\frac{2\\mu}{\\rho}}(v_{(i)}-\\frac{\\alpha_{(i)}}{\\rho})`\n#    and so that :math:`\\alpha_i=0`, we can focus on\n#    :math:`||v_{(i)}||_2`,\n\n#    .. math:: \\xi_i = ||v_{(i)}||_2.\n\n# Note that if each group contains only one predictor, the sacrifices\n# become the non-group ones.\n\n# .. _algorithm-4:\n\n# Algorithm\n# ~~~~~~~~~\n\n# Actually, the workflow is almost the same as non-group situation. We\n# just change the sacrifices in **Algorithm 2** to:\n\n# Algorithm 4: Group-splicing\n# ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n# 1. Input :math:`s,\\Sigma, \\mathcal{A}, \\mathcal{I}, \\alpha`;\n\n# 2. Compute forward sacrifices:\n#    :math:`\\zeta_{i} = ||\\alpha_{(i)}||_2, i\\in \\mathcal{I}` and backward\n#    sacrifices: :math:`\\xi_i = ||v_{(i)}||_2, i\\in \\mathcal{A}`;\n\n# 3. For :math:`k = 0, 1, \\cdots, \\min(s, p-s)` do:\n\n#       Exchange :math:`k` elements in :math:`\\mathcal{I}` with :math:`k`\n#       largest :math:`\\zeta` and in :math:`\\mathcal{A}` with :math:`k`\n#       smallest :math:`\\xi`;\n\n#       Form a normal PCA on active set to get :math:`v`;\n\n#       Re-compute :math:`v^T\\Sigma v`;\n\n#       Record the :math:`v_0 = \\arg\\max_v v^T\\Sigma v`;\n\n# 4. return :math:`v_0`.\n\n# Important Search\n# ----------------\n\n# Suppose that there are only a few variables are important (i.e. too many noise variables), \n# it may be a vise choice to focus on some important variables during splicing process. \n# This can save a lot of time, especially under a large $p$.\n\n# Algorithm\n# ~~~~~~~~~\n\n# Suppose we are focus on the sparsity level :math:`s` and we have the sacrifice :math:`\\zeta, \\xi`\n# from the last sparsity level's searching. Now we focus on an variables' subset :math:`U` with size `U\\_size`, \n# which is not larger than :math:`p`:\n\n# Algorithm : Important Search\n# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n#\n# 1. Input :math:`s, X, y, group\\_index, group\\_size, \\zeta, \\xi, U\\_size, max\\_iter`;\n#\n# 2. Sort all sacrifices and choose the largest :math:`U\\_size` variables as :math:`U`, initially;\n#\n# 3. For :math:`iter = 0, 1, \\cdots, max\\_iter` do:\n\n#       Mapping :math:`X, y, group\\_index, group\\_size` to `U`;\n\n#       Form splicing on this subset, until the active set is stable;\n\n#       Inverse mapping to full set;\n\n#       Re-compute the sacrifices with the new active set;\n\n#       Sort and update :math:`U` (similar to Step 2);\n\n#       If :math:`U` is unchanged (not in order), break;\n\n# 4. Return :math:`\\mathcal{A},  \\mathcal{I}`."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}