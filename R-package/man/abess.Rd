% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/abess.R
\name{abess.default}
\alias{abess.default}
\alias{abess}
\alias{abess.formula}
\title{Adaptive Best-Subset Selection via splicing algorithm}
\usage{
\method{abess}{default}(
  x,
  y,
  family = c("gaussian", "binomial"),
  tune.path = c("sequence", "gsection"),
  tune.type = c("gic", "ebic", "bic", "aic", "cv"),
  weight = rep(1, nrow(x)),
  normalize = NULL,
  c.max = 2,
  support.size = NULL,
  gs.range = NULL,
  always.include = NULL,
  max.splicing.iter = 20,
  screening.num = NULL,
  warm.start = TRUE,
  nfolds = 5,
  newton = c("exact", "approx"),
  newton.thresh = 1e-06,
  max.newton.iter = NULL,
  early.stop = FALSE,
  num.threads = 0,
  seed = 1,
  ...
)

\method{abess}{formula}(formula, data, subset, na.action, ...)
}
\arguments{
\item{x}{Input matrix, of dimension \eqn{n \times p}; each row is an observation
vector and each column is a predictor/feature/variable.}

\item{y}{The response variable, of \code{n} observations. 
For \code{family = "binomial"} should have two levels.}

\item{family}{One of the following models: \code{"gaussian"} and \code{"binomial"}.
Depending on the response. Any unambiguous substring can be given.}

\item{tune.path}{The method to be used to select the optimal support size. For
\code{method = "sequence"}, we solve the best subset selection problem for each size in \code{support.size}.
For \code{method = "gsection"}, we solve the best subset selection problem with support size ranged in \code{gs.range},
where the specific support size to be considered is determined by golden section.}

\item{tune.type}{The type of criterion for choosing the support size. 
Available options are \code{"gic"}, \code{"ebic"}, \code{"bic"}, \code{"aic"} and \code{"cv"}.
Default is \code{"gic"}.}

\item{weight}{Observation weights. Default is \code{1} for each observation.}

\item{normalize}{Options for normalization. \code{normalize = 0} for no normalization. 
\code{normalize = 1} for subtracting the mean of columns of \code{x}.
\code{normalize = 2} for scaling the columns of \code{x} to have \eqn{\sqrt n} norm.
\code{normalize = 3} for subtracting the means of the columns of \code{x} and \code{y}, and also
normalizing the columns of \code{x} to have \eqn{\sqrt n} norm.
If \code{normalize = NULL}, \code{normalize} will be set \code{1} for \code{"gaussian"},
\code{2} for \code{"binomial"}. Default is \code{normalize = NULL}.}

\item{c.max}{an integer splicing size. Default is: \code{c.max = 2}.}

\item{support.size}{An integer vector representing the alternative support sizes. 
Only used for \code{method = "sequence"}. Default is \code{1:min(n, round(n/(log(log(n))log(p))))}.}

\item{gs.range}{A integer vector with two elements. 
The first element is the minimum model size considered by golden-section, 
the later one is the maximum one. Default is \code{gs.range = c(1, min(n, round(n/(log(log(n))log(p)))))}.}

\item{always.include}{An integer vector containing the indexes of variables that should always be included in the model.}

\item{max.splicing.iter}{The maximum number of performing splicing algorithm. 
In most of the case, only a few splicings can guarantee the convergence. 
Default is \code{max.splicing.iter = 20}.}

\item{screening.num}{An integer number. Preserve \code{screening.num} number of predictors with the largest 
marginal maximum likelihood estimator before running algorithm.}

\item{warm.start}{Whether to use the last solution as a warm start. Default is \code{warm.start = TRUE}.}

\item{nfolds}{The number of folds in cross-validation. Default is \code{nfolds = 5}.
For variables in the same group, they should be located in adjacent columns of \code{x}
and their corresponding index in \code{group.index} should be the same.
Denote the first group as \code{1}, the second \code{2}, etc.
If you do not fit a model with a group structure,
please set \code{group.index = NULL}. Default is \code{NULL}.}

\item{newton}{A character specify the Newton's method for fitting generalized linear models, 
it should be either \code{newton = "exact"} or \code{newton = "approx"}.
If \code{newton = "exact"}, then the exact hessian is used, 
while \code{newton = "approx"} uses diagonal entry of the hessian, and can be faster.}

\item{newton.thresh}{a numerica value for controlling positive convergence tolerance. 
The Newton's iterations converge when \eqn{|dev - dev_{old}|/(|dev| + 0.1)<} \code{newton.thresh}.}

\item{max.newton.iter}{a integer giving the maximal number of Newton's iteration iterations.
Default is \code{max.newton.iter = 10} if \code{newton = "exact"}, and \code{max.newton.iter = 60} if \code{newton = "approx"}.}

\item{early.stop}{A boolean value decide whether early stoping. 
If \code{early.stop = TRUE}, algorithm will stop if the last tuning value less than the existing one. 
Default: \code{early.stop = FALSE}.}

\item{num.threads}{A integer decide the number of threads. 
If \code{num.threads = 0}, then all of available cores will be used. Default: \code{num.threads = 0}.}

\item{seed}{Seed to be used to devide the sample into cross-validation folds. Default is \code{seed = 1}.}

\item{...}{further arguments to be passed to or from methods.}

\item{formula}{an object of class "\code{formula}": 
a symbolic description of the model to be fitted. 
The details of model specification are given in the "Details" section of "\code{\link{formula}}".}

\item{data}{a data frame containing the variables in the \code{formula}.}

\item{subset}{an optional vector specifying a subset of observations to be used.}

\item{na.action}{a function which indicates 
what should happen when the data contain \code{NA}s. 
Defaults to \code{getOption("na.action")}.}
}
\value{
A \code{abess} class object, which is a \code{list} with the following components:
\item{best.model}{The best model chosen by algorithm. It is a \code{list} object comprising the following sub-components:
 1. \code{beta}: a fitted \eqn{p}-dimensional coefficients vector; 2. \code{coef0}: a numeric fitted intercept; 
 3. \code{support.index}: an index vector of best model's support set; 4. \code{support.size}: the support size of the best model; 
 5. \code{dev}: the deviance of the model; 6. \code{tune.value}: the tune value of the model.
}
\item{beta}{A \eqn{p}-by-\code{length(support.size)} matrix of coefficients, stored in column format.}
\item{coef0}{A Intercept vector of length \code{length(support.size)}.}
\item{tune.value}{A value of tuning criterion of length \code{length(support.size)}.}
\item{family}{Type of the model.}
\item{tune.path}{The path type for tuning parameters.}
\item{support.size}{The actual \code{support.size} values used. Note that it is not necessary the same as the input if the later have double values or duplicated values.} 
\item{tune.type}{The criterion type for tuning parameters.}
\item{screening.index}{The vector of indices selected by only feature screening. 
It would a empty numerical vector if \code{screening.num = 0}.}
\item{call}{The original call to \code{abess}.}
}
\description{
Perform adaptive best-subset selection for regression and binary classification in polynomial times.
}
\examples{
n <- 500
p <- 1500
support.size <- 3

################ linear model ################
dataset <- generate.data(n, p, support.size)
abess_fit <- abess(dataset[["x"]], dataset[["y"]])
abess_fit[["best.model"]]

################ logistic model ################
dataset <- generate.data(n, p, support.size, family = "binomial")
## use cross-validation to tuning
abess_fit <- abess(dataset[["x"]], dataset[["y"]], 
                   family = "binomial", tune.type = "cv")
abess_fit[["best.model"]]

################  Formula interface  ################
data("trim32")
abess_fit <- abess(y ~ ., data = trim32)
abess_fit
}
\references{
A polynomial algorithm for best-subset selection problem. Junxian Zhu, Canhong Wen, Jin Zhu, Heping Zhang, Xueqin Wang. Proceedings of the National Academy of Sciences Dec 2020, 117 (52) 33117-33123; DOI: 10.1073/pnas.2014241117

Sure independence screening for ultrahigh dimensional feature space. Fan, J. and Lv, J. (2008), Journal of the Royal Statistical Society: Series B (Statistical Methodology), 70: 849-911. https://doi.org/10.1111/j.1467-9868.2008.00674.x
}
\author{
Junxian Zhu, Canhong Wen, Jin Zhu, Heping Zhang, Xueqin Wang
}
