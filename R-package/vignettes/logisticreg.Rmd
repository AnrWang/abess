---
title: "logisticreg"
author: "胡励元 17306030"
date: "2021/5/27"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Smarket Dataset

We are going to apply best subset selection to the Titanic dataset obtained here https://www.kaggle.com/c/titanic/data. The training dataset consists of data about 889 passengers, and the goal of the competition is to predict the survival based on features including the class of service, the sex, the age etc. First, let's have a look at the dataset and exam if there is any missing data.

```{r}
dat <- read.csv('train.csv', header = T, na.strings=c(""))
head(dat)
dat <- dat[, c(2,3,5,6,7,8,10,12)]
sum(is.na(dat))
```
The `na.omit()` function allows us to delete the rows that contain any missing data. After that, we get a total of 714 samples left. 
```{r}
dat <- na.omit(dat)
dim(dat)
```
Then we change the factors into dummy variables with the `model.matrix()` function. Note that the `abess` function will automatically include the intercept.
```{r}
dat <- model.matrix(~., dat)[, -1]
dat <- as.data.frame(dat)
```

We split the dataset into a training set and a test set. The model is going to be built on the training set and later We will test the model performance on the test set.
```{r}
train <- dat[1:round((712*2)/3), ]
test <- dat[-(1:round((712*2)/3)), ]
```

## Best Subset Selection for Logistic Regression

The `abess()` function in the `abess` package allows you to perform best subset selection in a highly efficient way. You can call the `abess()` funtion using formula just like what you do with `lm()`. Or you can specify the design matrix `x` and the response `y`.

```{r}
library(abess)
abess_fit <- abess(Survived~., data = train, family = "binomial")
abess_fit <- abess(x = train[, -1], y = train$Survived, family = "binomial")
```

By default, the `abess` function implements the ABESS algorithm with the support size changing from 0 to $\min\{p,n/log(n)p \}$ and the best support size is determined by the Generalized Informatoin Criterion (GIC). You can change the tunging criterion by specifying the argument `tune.type`. The available tuning criterion now are `gic`, `aic`, `bic`, `ebic` and `cv`. For a quicker solution, you can change the tuning strategy to a golden section path which trys to find the elbow point of the tuning criterion over the hyperparameter space. Here we give an example.

```{r}
abess_fit.gs <- abess(Survived~., data = train, family = "binomial", tune = "bic", tune.path = "gs")
```

## Interprate the Result

Hold on, we aren't finished yet. After get the estimator, we can further do more exploring work.
The output of `abess()` function contains the best model for all the candidate support size in the `support.size`. You can use some generic function to quickly draw some information of those estimators.
```{r}
# draw the estimated coefficients on all candidate support size
coef(abess_fit)

# get the deviance of the estimated model on all candidate support size
deviance(abess_fit)

# print the fitted model
print(abess_fit)
```

The `plot.abess()` function helps to visualize the change of models with the change of support size. There are 5 types of graph you can generate, including `coef` for the coefficeint value, `l2norm` for the L2-norm of the coefficients, `dev` for the deviance and `tune` for the tuning value. Default if `coef`.
```{r}
plot(abess_fit, label=T)
```
The graph shows that, begining from the most dense model, the second variable (`Sex`) is included in the active set until the support size reaches 0.

We can also generate a graph about the tuning value. Remember that we used the default GIC to tune the support size. 
```{r}
plot(abess_fit, type="tune")
```

The tuning value reaches the lowest point at 4. And We might choose the estimated model with support size equals 6 as our final model. 

To extract any model from the `abess` object, we can call the `extract()` function with a given `support.size`. If `support.size` is not provided, the model with the best tuning value will be returned. Here we extract the model with support size equals 6.
```{r}
best.model = extract(abess_fit, support.size = 4)
str(best.model)
```

The return is a list containing the basic information of the estimated model.

## Make a Prediction

Prediction is allowed for all the estimated model. Just call `predict.abess()` function with the `support.size` set to the size of model you are interested in. If a `support.size` is not provided, prediction will be made on the model with best tuning value. The `predict.abess()` can provide both `link`, stands for the linear predictors, and the `response`, stands for the fitted probability. Here We will predict the probablity of survival on the `test.csv` data.
```{r}
fitted.results <- predict(abess_fit, newx = test, type = 'response')
```

If we chose 0.5 as the cut point, i.e, we predict the person survived the sinking of the Titanic if the fitted probablity is greater than 0.5, the accuracy will be 0.80.

```{r}
fitted.results <- ifelse(fitted.results > 0.5,1,0)
misClasificError <- mean(fitted.results != test$Survived)
print(paste('Accuracy',1-misClasificError))

```

We can also generate an ROC curve and calculate tha AUC value. On this dataset, the AUC is 0.87, which is quite close to 1.
```{r}
library(ROCR)
fitted.results <- predict(abess_fit, newx = test, type = 'response')
pr <- prediction(fitted.results, test$Survived)
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
plot(prf)
auc <- performance(pr, measure = "auc")
auc <- auc@y.values[[1]]
auc
```

