---
title: "An Introduction to `abess`"
author: "Jin Zhu"
output:
  pdf_document:
    fig_caption: yes
    toc: yes
    toc_depth: 3
vignette: >
  %\VignetteIndexEntry{An Introduction to abess}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

```{r, echo=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(warning = FALSE, eval = TRUE, message = FALSE)
```


## Briey introduction       
The R package `abess` implement a polynomial algorithm in the  [\color{blue}{paper}](https://www.pnas.org/content/early/2020/12/14/2014241117) for best-subset selection problem:
$$\min_{\beta \in \mathbb{R}^p} \frac{1}{2n} \| y - X\beta\|_2^2, \text{ subject to } \|\beta\|_0 \leq s,$$
where $\| \cdot \|_2$ is the $\ell_2$ norm, $\|\beta\|_0=\sum_{i=1}^pI( \beta_i\neq 0)$ is the $\ell_0$ norm of $\beta$, and the sparsity level $s$ is usually an unknown non-negative integer.
Next, we present an example to show how to use the **abess** package to solve a simple problem. 

## Quick example        
### Fixed support size best subset selection        
We generate a design matrix $X$ containing 300 observation and each observation has 1000 predictors. The response variable $y$ is linearly related to the first, second, and fifth predictors in $X$: 
$$y = 3X_1 + 1.5X_2 + 2X_5 + \epsilon,$$
where $\epsilon$ is a standard normal random variable. 
```{r}
library(abess)
synthetic_data <- generate.data(n = 300, p = 1000, 
                                beta = c(3, 1.5, 0, 0, 2, rep(0, 995)))
dat <- cbind.data.frame("y" = synthetic_data[["y"]], 
                        synthetic_data[["x"]])
dim(synthetic_data[["x"]])
head(synthetic_data[["y"]])
```

Then, we use the main function `abess` in the package to fit this dataset. 
By setting the arguments `support.size = s`, `abess` function conducts **Algorithm 1** in the [\color{blue}{paper}](https://www.pnas.org/content/early/2020/12/14/2014241117) for best-subset selection with a sparsity level `s`. In our example, we set the options: `support.size = 3`, and we run **Algorithm 1** with the following command: 
```{r}
abess_fit <- abess(y ~ ., data = dat, support.size = 3)
```
    
The output of `abess` comprises the selected best model: 
```{r}
str(abess_fit[["best.model"]])
```
The best model's support set is identical to the ground truth, and the coefficient estimation is the same as the oracle estimator given by `lm` function:
```{r}
lm(y ~ ., data = dat[, c(1, c(1, 2, 5) + 1)])
```

<!-- Users could `print`, `summary` or `predict` this bestmodel object just like working with classical regression modeling. This would be helpful for data scientists who are familiar with `lm` functions in R. -->

### Adaptive best subset selection        
Imaging we are unknown about the true sparsity level in real world data, and thus, we need to determine the most proper one. The **Algorithm 3** in the [\color{blue}{paper}](https://www.pnas.org/content/early/2020/12/14/2014241117) is designed for this scenario. `abess` is capable of performing this algorithm: 
```{r}
abess_fit <- abess(y ~ ., data = dat)
```

The output of `abess` also comprises the selected best model: 
```{r}
str(abess_fit[["best.model"]])
```
The output model accurately detect the true model size, which implies the **Algorithm 3** efficiently find both the optimal sparsity level and true effective predictors.


## Quick example        